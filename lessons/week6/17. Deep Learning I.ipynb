{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "0. Before we start\n",
    "\n",
    "1. Gradient Descend\n",
    "\n",
    "2. From derivatives to gradient: $n$-dimensional function minimization\n",
    "\n",
    "3. How to learn from data?\n",
    "\n",
    "    3.1 Square / Euclidean Loss\n",
    "    \n",
    "    3.2 Hinge / Margin Loss (i.e. Suport Vector Machines)\n",
    "    \n",
    "    3.3 Logistic Loss (Logistic Regression)\n",
    "    \n",
    "    3.4 Sigmoid Cross-Entropy Loss (Softmax classifier)\n",
    "    \n",
    "4. Batch Gradient Descend\n",
    "\n",
    "    4.1 Mini-batch Gradient Descent\n",
    "    \n",
    "5. Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4wmBVcRZgrT"
   },
   "source": [
    "## 0. Before we start\n",
    "\n",
    "The most simple thing we can try to minimize a function $f(x)$ would be to sample two points relatively near each other, and just repeatedly take a step down away from the largest value. This simple algorithm has a severe limitation: it can't get closer to the true minima than the step size.\n",
    "\n",
    "The **Nelder-Mead** method dynamically adjusts the step size based off the loss of the new point. If the new point is better than any previously seen value, it expands the step size to accelerate towards the bottom. Likewise if the new point is worse it contracts the step size to converge around the minima. The usual settings are to half the step size when contracting and double the step size when expanding.\n",
    "\n",
    "This method can be easily extended into higher dimensional examples, all that's required is taking one more point than there are dimensions. Then, the simplest approach is to replace the worst point with a point reflected through the centroid of the remaining n points. If this point is better than the best current point, then we can try stretching exponentially out along this line. On the other hand, if this new point isn't much better than the previous value, then we are stepping across a valley, so we shrink the step towards a better point.\n",
    "\n",
    "> See [\"An Interactive Tutorial on Numerical Optimization\"](http://www.benfrederickson.com/numerical-optimization/)\n",
    "\n",
    "Questions:\n",
    "\n",
    "+ What are the limitations of this method from a computational point of view?\n",
    "+ In which cases is a real alternative?\n",
    "\n",
    "**Gradient-based** optimization methods provide an alternative to sampling approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkZJe1Y-YjUL"
   },
   "source": [
    "## 1. Gradient Descend\n",
    "\n",
    "Let's suppose that we have a function $f(w): \\mathbf{R} \\rightarrow \\mathbf{R}$ and that our objective is to find the argument  $w$ that minimizes this function (for maximization, consider $-f(w)$). To this end, the critical concept is the *derivative*.\n",
    "\n",
    "The derivative of $f$ of a variable $w$, $ f'(w)$ or $\\frac{\\delta f}{\\delta  w}$,  is a measure of the rate at which the value of the function changes with respect to the change of the variable. It is defined as the following limit:\n",
    "\n",
    "\n",
    "$$ f'(w) = \\lim_{h \\rightarrow 0} \\frac{f(w + h) - f(w)}{h} $$\n",
    "\n",
    "The derivative specifies how to scale a small change in the input in order to obtain the corresponding change in the output. Knowing the value of $f(w)$ at a point $w$, this allows to predict the value of the function in a neighboring point:\n",
    "\n",
    "$$ f(w + h) \\approx f(w) + h f'(w)$$\n",
    "\n",
    "### First approach\n",
    "\n",
    "Then, by following these steps we can decrease the value of the function:\n",
    "\n",
    "+ Start from a random $w^0$ value.\n",
    "+ Compute the derivative $f'(w) = \\lim_{h \\rightarrow 0} \\frac{f(w + h) - f(w)}{h}$.\n",
    "+ Walk small steps in the opposite direction of the derivative, $w^{i+1} = w^i - h f'(w^i)$, because we know that $f(w - h f'(w))$ is less than $f(w)$ for  small enough $h$, until $ f'(w) \\approx 0$.\n",
    "\n",
    "The search for the minima ends when the derivative is zero because we have no more information about which direction to move. $w$ is called a critical or stationary point of $f(w)$ if $f'(w)=0$. \n",
    "\n",
    "All extrema points (maxima/minima) are critical points because $f(w)$ is lower/higher than at all neighboring points. But these are not the only critical points: there is a third class of critical points called *saddle points*. Saddle points are points that have partial derivatives equal to zero but at which the function has neither a maximum nor a minimum value.\n",
    "\n",
    "If $f$ is a *convex function*, when the derivative is zero this should be the extremum of our function. In other cases it could be a local minimum/maximum or a saddle point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1647971518729,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "5r_P-G3ZbZUm",
    "outputId": "ce0ad95d-72ec-4bcb-fbf3-508bcf322645"
   },
   "outputs": [],
   "source": [
    "# numerical derivative at a point x by using finite differences\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def fin_dif(x, \n",
    "            f, \n",
    "            h = 0.00001):\n",
    "    '''\n",
    "    This method returns the derivative of f at x\n",
    "    by using the finite difference method\n",
    "    '''\n",
    "    return (f(x+h) - f(x))/h\n",
    "\n",
    "x = 2.0\n",
    "print(\"{:2.4f}\".format(fin_dif(x,f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsBzquN2bgGn"
   },
   "source": [
    "> NOTE: It can be shown that the “centered difference formula\" is better when computing numerical derivatives:\n",
    "\n",
    "> $$ \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x - h)}{2h} $$\n",
    "The error in the \"finite difference\" approximation can be derived from Taylor's theorem and, assuming that $f$ is differentiable, is $O(h)$. In the case of “centered difference\" the error is $O(h^2)$.\n",
    "\n",
    "There are two problems with numerical derivatives:\n",
    "\n",
    "+ It is approximate.\n",
    "+ It is very slow to evaluate (two function evaluations: $f(x + h) , f(x - h)$ ).\n",
    "\n",
    "Our knowledge from Calculus could help!\n",
    "\n",
    "### Second approach\n",
    "\n",
    "To find the local minimum using gradient descend and in the case of knowing an analytical expression of the derivative of the **function** we wnat to minimize, you can start at a random point, and move into the direction of steepest descent relative to the derivative:\n",
    "\n",
    "+ Start from a random $x$ value.\n",
    "+ Compute the derivative $f'(x)$ analitically.\n",
    "+ Walk a small step in the opposite direction of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1647971519842,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "147fFQFFcd5Y",
    "outputId": "6f4d7be4-3df0-4a11-a937-013f39024406"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression \n",
    "from scipy import stats \n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "start = 15\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "plt.plot(x,y, 'r-')\n",
    "plt.plot([start],[start**2 - 6*start + 5],'o')\n",
    "ax.text(start,\n",
    "        start**2 - 6*start + 35,\n",
    "        'Start',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "\n",
    "d = 2 * start - 6\n",
    "end = start - d\n",
    "\n",
    "plt.plot([end],[end**2 - 6*end + 5],'o')\n",
    "plt.ylim([-10,250])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "ax.text(end,\n",
    "        start**2 - 6*start + 35,\n",
    "        'End',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['green'],\n",
    "       )\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnn1ivYNdIhp"
   },
   "source": [
    "**There is a problem! Which one?**\n",
    "\n",
    "We need to define a suitable *step* to modulate the value of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1645026903828,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "3dFqF4WVcqCG",
    "outputId": "d5359296-8dfd-488d-e997-7949524387b5"
   },
   "outputs": [],
   "source": [
    "old_min = 0\n",
    "temp_min = 15\n",
    "step_size = 0.01\n",
    "precision = 0.0001\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "    \n",
    "def f_derivative(x):\n",
    "    import math\n",
    "    return 2*x -6\n",
    "\n",
    "mins = []\n",
    "cost = []\n",
    "\n",
    "while abs(temp_min - old_min) > precision:\n",
    "    old_min = temp_min \n",
    "    gradient = f_derivative(old_min) \n",
    "    move = gradient * step_size\n",
    "    temp_min = old_min - move\n",
    "    cost.append((3-temp_min)**2)\n",
    "    mins.append(temp_min)\n",
    "\n",
    "# rounding the result to 2 digits because of the step size\n",
    "print(\"Local minimum occurs at {:3.6f}.\".format(round(temp_min,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1645026904377,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "ADdYLeGnc_Ae",
    "outputId": "9275d342-5f18-4cb5-9150-c23533e017b3"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "\n",
    "x, y = (zip(*enumerate(cost)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "plt.plot(x,y, 'r-', alpha=0.7)\n",
    "plt.ylim([-10,150])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1645026904737,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "AjBimEs5dEGB",
    "outputId": "e398f7ff-ab35-47df-c1b2-09e5c00430ae"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "plt.plot(x,y, 'r-')\n",
    "plt.ylim([-10,250])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.plot(mins,cost,'o', alpha=0.3)\n",
    "ax.text(start,\n",
    "        start**2 - 6*start + 25,\n",
    "        'Start',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "ax.text(mins[-1],\n",
    "        cost[-1]+20,\n",
    "        'End (%s steps)' % len(mins),\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j11Iabv98uP4"
   },
   "source": [
    "### Alpha\n",
    "\n",
    "The step size, **alpha**, is a slippy concept: if it is too small we will slowly converge to the solution, if it is too large we can diverge from the solution. \n",
    "\n",
    "There are several policies to follow when selecting the step size:\n",
    "\n",
    "+ Constant size steps. In this case, the size step determines the precision of the solution.\n",
    "+ Decreasing step sizes.\n",
    "+ At each step, select the optimal step.\n",
    "\n",
    "The last policy is good, but too expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVUkCGkqdRQP"
   },
   "source": [
    "## 2. From derivatives to gradient: $n$-dimensional function minimization\n",
    "Let's consider a $n$-dimensional function $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$. For example:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sum_{n} x_n^2$$\n",
    "Our objective is to find the argument  $\\mathbf{x}$ that minimizes this function.\n",
    "\n",
    "The gradient of $f$ is the vector whose components are the $n$ partial derivatives of $f$. It is thus a vector-valued function.\n",
    "\n",
    "The gradient points in the direction of the greatest rate of increase of the function.\n",
    "\n",
    "$$\\nabla {f} = (\\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1645026904738,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "UTjC31WKdlC0",
    "outputId": "7b76173f-2aee-4cbb-938f-27045c77a3e5"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return sum(x_i**2 for x_i in x)\n",
    "\n",
    "def fin_dif_partial_centered(x, \n",
    "                             f, \n",
    "                             i, \n",
    "                             h=1e-6):\n",
    "    '''\n",
    "    This method returns the partial derivative of the i-th \n",
    "    component of f at x\n",
    "    by using the centered finite difference method\n",
    "    '''\n",
    "    w1 = [x_j + (h if j==i else 0) for j, x_j in enumerate(x)]\n",
    "    w2 = [x_j - (h if j==i else 0) for j, x_j in enumerate(x)]\n",
    "    return (f(w1) - f(w2))/(2*h)\n",
    "\n",
    "def gradient_centered(x, \n",
    "                      f, \n",
    "                      h=1e-6):\n",
    "    '''\n",
    "    This method returns the gradient vector of f at x\n",
    "    by using the centered finite difference method\n",
    "    '''\n",
    "    return[round(fin_dif_partial_centered(x,f,i,h), 10) for i,_ in enumerate(x)]\n",
    "\n",
    "\n",
    "x = [1.0,1.0,1.0]\n",
    "\n",
    "print('{:.6f}'.format(f(x)), gradient_centered(x,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuoyRktk7f5z"
   },
   "source": [
    "The function we have evaluated, $f({\\mathbf x}) = x_1^2+x_2^2+x_3^2$, is $3$ at $(1,1,1)$ and the gradient vector at this point is $(2,2,2)$. \n",
    "\n",
    "Then, we can follow this steps to maximize (or minimize) the function:\n",
    "\n",
    "+ Start from a random $\\mathbf{x}$ vector.\n",
    "+ Compute the gradient vector.\n",
    "+ Walk a small step in the opposite direction of the gradient vector.\n",
    "\n",
    "> It is important to be aware that this gradient computation is very expensive: if $\\mathbf{x}$ has dimension $n$, we have to evaluate $f$ at $2*n$ points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQcpRA91XWim"
   },
   "source": [
    "## 3. How to learn from data?\n",
    "\n",
    "In general, *Learning from Data* is a scientific discipline that is concerned with the design and development of algorithms that allow computers to infer, from data, a model that allows a compact representation of raw data and/or good generalization abilities. In the former case we are talking about non supervised learning. In the later, supervised learning. \n",
    "\n",
    "This is nowadays an important technology because it enables computational systems to improve their performance with experience accumulated from the observed data in real world scenarios.\n",
    "\n",
    "Let's consider the supervised learning problem from an optimization point of view. When learning a model from data the most common scenario is composed of the following elements:\n",
    "\n",
    "+ A dataset $(\\mathbf{x},y)$ of $n$ examples. For example, $(\\mathbf{x},y)$ can represent:\n",
    "\n",
    "  + $\\mathbf{x}$: the behavior of a game player; $y$: monthly payments.\n",
    "  + $\\mathbf{x}$: sensor data about your car engine; $y$: probability of engine error.\n",
    "  + $\\mathbf{x}$: financial data of a bank customer; $y$: customer rating.\n",
    "\n",
    "  If $y$ is a real value, the problem we are trying to solve is called a *regression* problem. If $y$ is binary or categorical, it is called a *classification* problem.\n",
    "\n",
    "+ A target function $f_{(\\mathbf{x},y)}(\\mathbf{w})$, that we want to minimize, representing the discrepancy between our data and the model we want to fit.\n",
    "\n",
    "+ A model $M$ that is represented by a set of parameters $\\mathbf{w}$.\n",
    "\n",
    "+ The gradient of the target function, $\\nabla {f_{(\\mathbf{x},y)}(\\mathbf{w})}$ with respect to model parameters.\n",
    "\n",
    "In the case of regression $f_{(\\mathbf{x},y)}(\\mathbf{w})$ represents the errors from a data representation model $M$. Fitting a model can be defined as finding the optimal parameters $\\mathbf{w}$ that minimize the following expression:\n",
    "\n",
    "$f_{(\\mathbf{x},y)}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i} (y_i - M(\\mathbf{x}_i,\\mathbf{w}))^2$\n",
    "\n",
    "Alternative regression and classification problems can be defined by considering different formulations to measure the errors from a data representation model. These formulations are known as the *Loss Function* of the problem. \n",
    "\n",
    "\n",
    "### 3.1 Square / Euclidean Loss\n",
    "\n",
    "In regression problems, the most common loss function is the square loss function:\n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i (y_i - f(\\mathbf{x}_i))^2  $$\n",
    "\n",
    "The square loss function can be re-written and utilized for classification:\n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i (1 - y_i f(\\mathbf{x}_i))^2  $$\n",
    "\n",
    "\n",
    "### 3.2 Hinge / Margin Loss (i.e. Suport Vector Machines)\n",
    "\n",
    "The hinge loss function is defined as:\n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i \\mbox{max}(0, 1 - y_i f(\\mathbf{x}_i))  $$\n",
    "\n",
    "The hinge loss provides a relatively tight, convex upper bound on the 0–1 Loss.\n",
    "\n",
    "### 3.3 Logistic Loss (Logistic Regression)\n",
    "\n",
    "This function displays a similar convergence rate to the hinge loss function, and since it is continuous, simple gradient descent methods can be utilized. \n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} log(1 + exp(-y_i f(\\mathbf{x}_i))) $$\n",
    "\n",
    "\n",
    "### 3.4 Sigmoid Cross-Entropy Loss (Softmax classifier)\n",
    "\n",
    "Cross-Entropy is a loss function that is very used for training **multiclass problems**. We'll focus on models that assume that classes are mutually exclusive. \n",
    "\n",
    "In this case, our labels have this form $\\mathbf{y}_i =(1.0,0.0,0.0)$. If our model predicts a different distribution, say  $ f(\\mathbf{x}_i)=(0.4,0.1,0.5)$, then we'd like to nudge the parameters so that $f(\\mathbf{x}_i)$ gets closer to $\\mathbf{y}_i$.\n",
    "\n",
    "C.Shannon showed that if you want to send a series of messages composed of symbols from an alphabet with distribution $y$ ($y_j$  is the probability of the  $j$-th symbol), then to use the smallest number of bits on average, you should assign  $\\log(\\frac{1}{y_j})$  bits to the  $j$-th symbol. \n",
    "\n",
    "The optimal number of bits is known as **entropy**:\n",
    "\n",
    "$$ H(\\mathbf{y}) = \\sum_j y_j \\log\\frac{1}{y_j} = - \\sum_j y_j \\log y_j$$\n",
    "\n",
    "**Cross entropy** is the number of bits we'll need if we encode symbols by using a wrong distribution $\\hat y$:\n",
    "\n",
    "$$ H(y, \\hat y) =   - \\sum_j y_j \\log \\hat y_j $$ \n",
    "\n",
    "In our case, the real distribution is $\\mathbf{y}$ and the \"wrong\" one is $f(\\mathbf{x}_i)$. So, minimizing **cross entropy** with respect our model parameters will result in the model that best approximates our labels if considered as a probabilistic distribution. \n",
    "\n",
    "Cross entropy is used in combination with **Softmax** classifier. In order to classify $\\mathbf{x}_i$ we could take the index corresponding to the max value of $f(\\mathbf{x}_i)$, but Softmax gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation:\n",
    "\n",
    "$$ P(\\mathbf{y}_i = j \\mid \\mathbf{x_i}) = - log \\left( \\frac{e^{f_j(\\mathbf{x_i})}}{\\sum_k e^{f_k(\\mathbf{x_i})} } \\right) $$\n",
    "\n",
    "where $f_k$ is a linear classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Llc1-3D19CjX"
   },
   "source": [
    "## 4. Batch gradient descend\n",
    "\n",
    "We can implement **gradient descend** in the following way (*batch gradient descend*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1645026904738,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "xPHgrfHE9EaE",
    "outputId": "9e84da69-ef06-4c7b-8bab-d15036c549e3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# f = 2x\n",
    "x = np.arange(10)\n",
    "y = np.array([2*i for i in x])\n",
    "\n",
    "# f_target = 1/n Sum (y - wx)**2\n",
    "def target_f(x,y,w):\n",
    "    return np.sum((y - x * w)**2.0) / x.size\n",
    "\n",
    "# gradient_f = 2/n Sum 2wx**2 - 2xy\n",
    "def gradient_f(x,y,w):\n",
    "    return 2 * np.sum(2*w*(x**2) - 2*x*y) / x.size\n",
    "\n",
    "def step(w,grad,alpha):\n",
    "    return w - alpha * grad\n",
    "\n",
    "\n",
    "\n",
    "def BGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        y, \n",
    "        toler = 1e-6, \n",
    "        alpha=0.01):\n",
    "    '''\n",
    "    Batch gradient descend by using a given step\n",
    "    '''\n",
    "    w = random.random()\n",
    "    val = target_f(x,y,w)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        gradient = gradient_f(x,y,w)\n",
    "        next_w = step(w, gradient, alpha)\n",
    "        next_val = target_f(x,y,next_w)    \n",
    "        if (abs(val - next_val) < toler):\n",
    "            return w\n",
    "        else:\n",
    "            w, val = next_w, next_val\n",
    "            \n",
    "print('{:.6f}'.format(BGD(target_f, gradient_f, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW7el7av9ine"
   },
   "source": [
    "#### Stochastic Gradient Descend\n",
    "\n",
    "The last function evals the whole dataset $(\\mathbf{x}_i,y_i)$ at every step. \n",
    "\n",
    "If the dataset is large, this strategy is too costly. In this case we will use a strategy called **SGD** (*Stochastic Gradient Descend*).\n",
    "\n",
    "When learning from data, the cost function is additive: it is computed by adding sample reconstruction errors. \n",
    "\n",
    "Then, we can compute the estimate the gradient (and move towards the minimum) by using only **one data sample** (or a small data sample).\n",
    "\n",
    "Thus, we will find the minimum by iterating this gradient estimation over the dataset.\n",
    "\n",
    "A full iteration over the dataset is called **epoch**. During an epoch, data must be used in a random order.\n",
    "\n",
    "If we apply this method we have some theoretical guarantees to find a good minimum:\n",
    "+ SGD essentially uses the inaccurate gradient per iteration. Since there is no free food, what is the cost by using approximate gradient? The answer is that the convergence rate is slower than the gradient descent algorithm.\n",
    "+ The convergence of SGD has been analyzed using the theories of convex minimization and of stochastic approximation: it converges almost surely to a global minimum when the objective function is convex or pseudoconvex, and otherwise converges almost surely to a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1645026904739,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "enqYrpSq9kDI",
    "outputId": "ebd4bd87-ffd1-4d6a-d9a5-705f09204ea7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(10)\n",
    "y = np.array([2*i for i in x])\n",
    "data = list(zip(x,y))\n",
    "\n",
    "for (x_i,y_i) in data:\n",
    "    print('{:3d} {:3d}'.format(x_i,y_i))\n",
    "print(\"\")\n",
    "\n",
    "def in_random_order(data):\n",
    "    '''\n",
    "    Random data generator\n",
    "    '''\n",
    "    import random\n",
    "    indexes = [i for i,_ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "        \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def SGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        y, \n",
    "        toler = 1e-6, \n",
    "        epochs=100, \n",
    "        alpha_0=0.01):\n",
    "    '''\n",
    "    Stochastic gradient descend with automatic step adaptation (by\n",
    "    reducing the step to its 95% when there are iterations with no increase)\n",
    "    '''\n",
    "    data = list(zip(x,y))\n",
    "    w = random.random()\n",
    "    alpha = alpha_0\n",
    "    min_w, min_val = float('inf'), float('inf')\n",
    "    epoch = 0\n",
    "    iteration_no_increase = 0\n",
    "    while epoch < epochs and iteration_no_increase < 100:\n",
    "        val = target_f(x, y, w)\n",
    "        if min_val - val > toler:\n",
    "            min_w, min_val = w, val\n",
    "            alpha = alpha_0\n",
    "            iteration_no_increase = 0\n",
    "        else:\n",
    "            iteration_no_increase += 1\n",
    "            alpha *= 0.95\n",
    "        for x_i, y_i in list(in_random_order(data)):\n",
    "            gradient_i = gradient_f(x_i, y_i, w)\n",
    "            w = w - (alpha *  gradient_i)\n",
    "        epoch += 1\n",
    "    return min_w\n",
    "  \n",
    "print('w: {:.6f}'.format(SGD(target_f, gradient_f, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6FcMBx497LP"
   },
   "source": [
    "### 4.1 Mini-batch Gradient Descent\n",
    "\n",
    "In code, general batch gradient descent looks something like this:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "    grad = evaluate_gradient(target_f, data, w)\n",
    "    w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "For a pre-defined number of epochs, we first compute the gradient vector of the target function for the whole dataset w.r.t. our parameter vector. \n",
    "\n",
    "**Stochastic gradient descent** (SGD) in contrast performs a parameter update for each training example and label:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for sample in data:\n",
    "        grad = evaluate_gradient(target_f, sample, w)\n",
    "        w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of $n$ training examples:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for batch in get_batches(data, batch_size=50):\n",
    "    grad = evaluate_gradient(target_f, batch, w)\n",
    "    w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "Minibatch SGD has the advantage that it works with a slightly less noisy estimate of the gradient. However, as the minibatch size increases, the number of updates done per computation done decreases (eventually it becomes very inefficient, like batch gradient descent). \n",
    "\n",
    "There is an optimal trade-off (in terms of computational efficiency) that may vary depending on the data distribution and the particulars of the class of function considered, as well as how computations are implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTK8mzlM_yGN"
   },
   "source": [
    "## 5. Automatic Differentiation\n",
    "\n",
    "> The **backpropagation** algorithm was originally introduced in the 1970s, but its importance wasn't fully appreciated until a famous 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams. (Michael Nielsen in \"Neural Networks and Deep Learning\", http://neuralnetworksanddeeplearning.com/chap2.html).\n",
    "\n",
    "> **Backpropagation** is the key algorithm that makes training deep models computationally tractable. For modern neural networks, it can make training with gradient descent as much as ten million times faster, relative to a naive implementation. That’s the difference between a model taking a week to train and taking 200,000 years. (Christopher Olah, 2016)\n",
    "\n",
    "We have seen that in order to optimize our models we need to compute the derivative of the loss function with respect to all model paramaters. \n",
    "\n",
    "The computation of derivatives in computer models is addressed by four main methods: \n",
    "\n",
    "+ manually working out derivatives and coding the result (as in the original paper describing backpropagation); \n",
    "+ numerical differentiation (using finite difference approximations); \n",
    "+ symbolic differentiation (using expression manipulation in software, such as Sympy); \n",
    "+ and automatic differentiation (AD).\n",
    "\n",
    "**Automatic differentiation** (AD) works by systematically applying the **chain rule** of differential calculus at the elementary operator level.\n",
    "\n",
    "Let $ y = f(g(x)) $ our target function. In its basic form, the chain rule states:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial g} \\frac{\\partial g}{\\partial x} $$\n",
    "\n",
    "or, if there are more than one variable $g_i$ in-between $y$ and $x$ (f.e. if $f$ is a two dimensional function such as $f(g_1(x), g_2(x))$), then:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\sum_i \\frac{\\partial y}{\\partial g_i} \\frac{\\partial g_i}{\\partial x} $$\n",
    "\n",
    "> See https://www.math.hmc.edu/calculus/tutorials/multichainrule/\n",
    "\n",
    "Now, let's see how AD allows the accurate evaluation of derivatives at machine precision, with only a small constant factor of overhead.\n",
    "\n",
    "In its most basic description, AD relies on the fact that all numerical computations\n",
    "are ultimately compositions of a finite set of elementary operations for which derivatives are known.\n",
    "\n",
    "For example, let's consider the computation of the derivative of this function, that represents a 1-layer neural network model:\n",
    "\n",
    "$$\n",
    "    f(x) = \\frac{1}{1 + e^{- ({w}^T \\cdot  x + b)}} \n",
    "$$\n",
    "\n",
    "\n",
    "First, let's write how to evaluate $f(x)$ via a sequence of primitive operations:\n",
    "\n",
    "\n",
    "```python\n",
    "x = ?\n",
    "f1 = w * x\n",
    "f2 = f1 + b\n",
    "f3 = -f2\n",
    "f4 = 2.718281828459 ** f3\n",
    "f5 = 1.0 + f4\n",
    "f = 1.0/f5\n",
    "```\n",
    "\n",
    "The question mark indicates that $x$ is a value that must be provided. \n",
    "\n",
    "This *program* can compute the value of $x$ and also **populate program variables**. \n",
    "\n",
    "We can evaluate $\\frac{\\partial f}{\\partial x}$ at some $x$ by using the chain rule. This is called *forward-mode differentiation*. \n",
    "\n",
    "In our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1645026904739,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "KflHgUbk_992",
    "outputId": "c90b6bb1-63ab-4a58-b1cb-1c4c045c7288"
   },
   "outputs": [],
   "source": [
    "def f(x,w,b):\n",
    "    f1 = w * x\n",
    "    f2 = f1 + b\n",
    "    f3 = -f2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    f5 = 1.0 + f4\n",
    "    return 1.0/f5\n",
    "\n",
    "def dfdx_forward(x, w, b):\n",
    "    f1 = w * x\n",
    "    p1 = w                            # p1 = df1/dx\n",
    "    f2 = f1 + b\n",
    "    p2 = p1 * 1.0                     # p2 = p1 * df2/df1 \n",
    "    f3 = -f2\n",
    "    p3 = p2 * -1.0                    # p3 = p2 * df3/df2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    p4 = p3 * 2.718281828459 ** f3    # p4 = p3 * df4/df3\n",
    "    f5 = 1.0 + f4\n",
    "    p5 = p4 * 1.0                     # p5 = p4 * df5/df4\n",
    "    f = 1.0/f5\n",
    "    df = p5 * -1.0 / f5 ** 2.0        # df/dx = p5 * df/df5\n",
    "    return f, df\n",
    "\n",
    "der = (f(3+0.00001, 2, 1) - f(3, 2, 1))/0.00001\n",
    "\n",
    "print(\"Value of the function at (3, 2, 1): \",f(3, 2, 1))\n",
    "print(\"df/dx Derivative (fin diff) at (3, 2, 1): \",der)\n",
    "print(\"df/dx Derivative (aut diff) at (3, 2, 1): \",dfdx_forward(3, 2, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O71IkVfjAWs0"
   },
   "source": [
    "It is interesting to note that this *program* can be automatically derived  if we have access to **subroutines implementing the derivatives of primitive functions** (such as $\\exp{(x)}$ or $1/x$) and all intermediate variables are computed in the right order. \n",
    "\n",
    "It is also interesting to note that AD allows the accurate evaluation of derivatives at **machine precision**, with only a small constant factor of overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_IbCnklAdG1"
   },
   "source": [
    "Forward differentiation is efficient for functions $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with $n << m$ (only $O(n)$ sweeps are necessary). \n",
    "\n",
    "For cases $n >> m$ a different technique is needed. To this end, we will rewrite the chain rule as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial x} \\frac{\\partial f}{\\partial g}\n",
    "$$\n",
    "\n",
    "to propagate derivatives backward from a given output. This is called *reverse-mode differentiation*. Reverse pass starts at the end (i.e. $\\frac{\\partial f}{\\partial f} = 1$) and propagates backward to all dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1645026904739,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "dpaRdXQsARE2",
    "outputId": "de8b5d9e-2d29-4840-b161-f4b471d870e9"
   },
   "outputs": [],
   "source": [
    "def dfdx_backward(x, w, b):\n",
    "    import numpy as np\n",
    "    f1 = w * x\n",
    "    f2 = f1 + b\n",
    "    f3 = -f2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    f5 = 1.0 + f4\n",
    "    f = 1.0/f5\n",
    "    \n",
    "    pf = 1.0                           # pf = df/df\n",
    "    p5 = 1.0 * -1.0 / (f5 * f5) * pf   # p5 = pf * df/df5 \n",
    "    p4 = p5 * 1.0                      # p4 = p5 * df5/df4\n",
    "    p3 = p4 * np.log(2.718281828459) \\\n",
    "          * 2.718281828459 ** f3       # p3 = p4 * df4/df3\n",
    "    p2 = p3 * -1.0                     # p2 = p3 * df3/df2\n",
    "    p1 = p2 * 1.0                      # p1 = p2 * df2/df1\n",
    "    dfx = p1 * w                       # dfx = p1 * df1/dx \n",
    "    return f, dfx\n",
    "\n",
    "print(\"df/dx Derivative (aut diff) at (3, 2, 1): \",\n",
    "      dfdx_backward(3, 2, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r88BQg4yA44Q"
   },
   "source": [
    "Any complex function that can be decomposed in a set of elementary functions can be derived in an automatic way, at machine precision, by this algorithm!\n",
    "\n",
    "**We no longer need to code complex derivatives to apply SGD!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Learning from data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
