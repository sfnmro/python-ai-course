{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 4: Non-linear models and ensemble learning\n",
    "\n",
    "1. Non-linear models\n",
    "\n",
    "    1.1 Nearest neighbors\n",
    "\n",
    "    1.2 Decision Trees\n",
    "    \n",
    "    1.3 Extending Support Vector Machines to the Non-Linear Case \n",
    "\n",
    "\n",
    "2. Ensemble learning\n",
    "    \n",
    "    2.1 Introduction to ensemble learning\n",
    "    \n",
    "    2.2 Bagging and Random Forest\n",
    "    \n",
    "    2.3 Introduction to the multiclass problem and Error Correcting Output Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Non-linear models in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our last case study, we shown that linear models can achive very good results. This is particularly true in datasets with large dimensionality. In those scenarios, the ratio between data and number of dimensions can be quite small. As a result we have a set of data spread out and it is very likely a linear model will be enough. However, it we have large amounts of data, this will not be enough. In those scenarios we may have to use non-linear models. \n",
    "\n",
    "In this notebook, we will introduce two concepts non-linearity and multi-class. Three models will be covered, Nearest Neighbors, Decision Trees and Kernel learning. We will apply these models to the predict **Customer Churn**. \n",
    "\n",
    "Let us first introduce the case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE STUDY: Customer Churn Analysis\n",
    "\n",
    "Modeling churn means to understand what keeps the customer engaged to our product. Its analysis goal is to predict or describe the **churn rate** i.e. the rate at which customer leave or cease the subscription to a service. Its value lies in the fact that engaging new customers is often more costly than retaining existing ones. For that reason subscription business-based companies usually have proactive policies towards customer retention.\n",
    "\n",
    "In this case study, we aim at building a machine learning based model for customer churn prediction on data from a Telecom company. Each row on the dataset represents a subscribing telephone customer. Each column contains customer attributes such as phone number, call minutes used during different times of day, charges incurred for services, lifetime account duration, and whether or not the customer is still a customer.\n",
    "\n",
    "This case is partially inspired in Eric Chiang's analysis of churn rate. Data is available from the University of California Irvine machine learning repositories data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete set of attributes is the following:\n",
    "\n",
    "+ State: categorical, for the 50 states and the District of Columbia\n",
    "+ Account length: integer-valued, how long an account has been active \n",
    "+ Area code: categorical\n",
    "+ Phone number: customer ID\n",
    "+ International Plan: binary feature, yes or no\n",
    "+ VoiceMail Plan: binary feature, yes or no\n",
    "+ Number of voice mail messages: integer-valued\n",
    "+ Total day minutes: continuous, minutes customer used service during the day\n",
    "+ Total day calls: integer-valued\n",
    "+ Total day charge: continuous\n",
    "+ Total evening minutes: continuous, minutes customer used service during the evening\n",
    "+ Total evening calls: integer-valued\n",
    "+ Total evening charge: continuous\n",
    "+ Total night minutes: continuous, minutes customer used service during the night\n",
    "+ Total night calls: integer-valued\n",
    "+ Total night charge: continuous\n",
    "+ Total international minutes: continuous, minutes customer used service to make international calls\n",
    "+ Total international calls: integer-valued\n",
    "+ Total international charge: continuous\n",
    "+ Number of calls to customer service: integer-valued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "['State', 'Account Length', 'Area Code', 'Phone', \"Int'l Plan\", 'VMail Plan', 'VMail Message', 'Day Mins', 'Day Calls', 'Day Charge', 'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls', 'Night Charge', 'Intl Mins', 'Intl Calls', 'Intl Charge', 'CustServ Calls', 'Churn?']\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Area Code</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Int'l Plan</th>\n",
       "      <th>VMail Plan</th>\n",
       "      <th>Night Charge</th>\n",
       "      <th>Intl Mins</th>\n",
       "      <th>Intl Calls</th>\n",
       "      <th>Intl Charge</th>\n",
       "      <th>CustServ Calls</th>\n",
       "      <th>Churn?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>382-4657</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>371-7191</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>358-1921</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>375-9999</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>330-6626</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AL</td>\n",
       "      <td>118</td>\n",
       "      <td>510</td>\n",
       "      <td>391-8027</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>9.18</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Account Length  Area Code     Phone Int'l Plan VMail Plan  \\\n",
       "0    KS             128        415  382-4657         no        yes   \n",
       "1    OH             107        415  371-7191         no        yes   \n",
       "2    NJ             137        415  358-1921         no         no   \n",
       "3    OH              84        408  375-9999        yes         no   \n",
       "4    OK              75        415  330-6626        yes         no   \n",
       "5    AL             118        510  391-8027        yes         no   \n",
       "\n",
       "   Night Charge  Intl Mins  Intl Calls  Intl Charge  CustServ Calls  Churn?  \n",
       "0         11.01       10.0           3         2.70               1  False.  \n",
       "1         11.45       13.7           3         3.70               1  False.  \n",
       "2          7.32       12.2           5         3.29               0  False.  \n",
       "3          8.86        6.6           7         1.78               2  False.  \n",
       "4          8.41       10.1           3         2.73               3  False.  \n",
       "5          9.18        6.3           6         1.70               0  False.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "churn_df = pd.read_csv('./datasets/churn.csv')\n",
    "col_names = churn_df.columns.tolist()\n",
    "\n",
    "print (\"Column names:\")\n",
    "print (col_names)\n",
    "\n",
    "to_show = col_names[:6] + col_names[-6:]\n",
    "\n",
    "print (\"\\nSample data:\")\n",
    "churn_df[to_show].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chrun_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mchrun_df\u001b[49m\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39minfo(),data\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chrun_df' is not defined"
     ]
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Isolate target data\n",
    "churn_result = churn_df['Churn?']\n",
    "y = np.where(churn_result == 'True.',1,0)\n",
    "\n",
    "# We don't need these columns\n",
    "to_drop = ['State','Phone','Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop,axis=1) #X\n",
    "\n",
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
    "\n",
    "# Pull out features for future use\n",
    "features = churn_feat_space.columns\n",
    "\n",
    "X = churn_feat_space.values.astype(np.float)\n",
    "\n",
    "print (\"Feature space holds %d observations and %d features\" % X.shape)\n",
    "print (\"Unique target labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.pie(np.c_[len(y)-np.sum(y),np.sum(y)][0],labels=['No Churn','Churn'],colors=['green','pink'],shadow=True,autopct ='%.2f' )\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data for future use.\n",
    "import pickle\n",
    "ofname = open('churn_data.pkl', 'wb')\n",
    "s = pickle.dump([X,y,features],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** This kind of datasets are called **unbalanced** datasets. Name a trivial classifier with \"good\" accuracy in this data set?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced datasets**\n",
    "<p>\n",
    "The unbalanced term describes the condition of the data where the ratio between the sizes of the positive and negative is a small value. In those scenarios, always predicting the majority class usually yields good accuracy performance, though it is ill informative. This kind of problems is very common when we want to model unusual events such as rare diseases, the occurrence of a failure in machinery, credit card fraud operations, etc. In those scenarios gathering data from usual events is very easy but collecting data from unusual events is difficult and results in a comparatively small size data set. In order to measure the performance on those data sets one has to use other performance metrics, such as specificity or positive predictive value on the minority class. In the end, the value of a misclassification of a sample depends on the application and the user. For example, in cancer detection because the cost of missing one patient in a trial is very large, we want the predictor to have very large sensitivity (we do not accept false negatives) though it means accepting more false positives. These false positives can be discarded in subsequent tests. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbors is a member of the instance based learning and lazy Learning families. Instance based models base the model on the evaluation of a function that depends on the point we are querying and training data. Nearest Neighbors is the **simplest** of these techniques. The rationale behind this model is as follows: Each training data set can be seen as a solved case/problem. Thus, given a new problem instance we may retrieve the most *similar* case in our data set and apply the same solution. In the case of classification, this means that we select the label of the most similar data example in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the boundary looks like in a toy problem.\n",
    "\n",
    "MAXN=100\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "\n",
    "#Evaluate the model for a given weight\n",
    "clf = neighbors.KNeighborsClassifier(11)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "+ The boundary is piece-wise linear. It is composed of edges of the Voronoi diagram.\n",
    "+ Observe that the classifier perfectly fits the training data. Adding or removing one data point can largely change the boundary. This implies that the complexity of the method is large.\n",
    "+ The key component of the nearest neigbors classifier is the notion of similarity/distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that regularization explicitly models complexity. Regularization is usually a penalty term. In nearest neighbors we can penalize solutions with small \"support\" by using a majority voting on the $k$ closests data samples to the query sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the boundary looks like in a toy problem.\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(3)\n",
    "clf.fit(X,y.ravel())\n",
    "Z2=clf.predict(data)\n",
    "Z2.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.4, vmin=-1, vmax=1)\n",
    "plt.imshow(Z2, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.2, vmin=-1, vmax=1)\n",
    "\n",
    "plt.contour(XX,YY,Z2,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Churn classification with nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us head back to analyzing the problem of customer churn prediction. We may fit a 1-Nearest Neighbor classifier and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "kf=model_selection.KFold(n_splits=5)\n",
    "kf.get_n_splits()\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    dt.fit(X_train,y_train)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a bad result. Remember that by always selecting class 'no churn' we should get around $85\\%$ of accuracy. As it was noticed before the definition of distance is critical. In NN we are using Euclidean distance. Distances assume that all variables operate at the same scale, i.e. all are commensurable. A change in one unit in one of the variables is equivalent to/as important as a change of 1 unit in the other. In this data set, this does not happen. For example, area codes values are around 400 while whether the customer enjoys an international plan take values 0 and 1. Thus, we may account for these changes by scaling the features. The most standard way of doing this is feature normalization or standarization. In this preprocessing technique each feature is scaled to have zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "acc_snooping = np.zeros((5,))\n",
    "i=0\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits()\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt = neighbors.KNeighborsClassifier(1)\n",
    "    dt.fit(X_train,y_train)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc_snooping[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc_snooping)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** In the former process we have accidentally snooped into the data and the result is contaminated. Where?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    dt = neighbors.KNeighborsClassifier(1)\n",
    "    dt.fit(X_train,y_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct=np.c_[acc_snooping,acc]\n",
    "plt.boxplot(acct);\n",
    "for i in range(2):\n",
    "    xderiv = (i+1)*np.ones(acct[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acct[:,i],'ro',alpha=0.3)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['snooping', 'no snooping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A brief description of the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is much better. As we have seen accuracy can be a little informative in some problems. For this reason we may use other performance measures. Classic performance measures can be derived from the confusion matrix. Consider the following confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    ax.text(0, 0, 'TP', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(0, 1, 'FN', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(1, 0, 'FP', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(1, 1, 'TN', va='center', ha='center',color='white',size=20)            \n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is divided in four quarters and contains\n",
    "\n",
    "+ True Positives (TP): Positive samples predicted as such.\n",
    "+ True Negatives (TN): Negative samples predicted as such.\n",
    "+ False Positives (FP): Negative samples predicted as positive.\n",
    "+ False Negatives (FN): Positive samples predicted as negative.\n",
    "\n",
    "The combination of these elements allows to define several performance metrics:\n",
    "\n",
    "+ Accuracy: \n",
    "\n",
    "$$\\text{accuracy}=\\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}$$\n",
    "\n",
    "Column-wise we find these two partial performance metrics:\n",
    "\n",
    "+ Sensitivity or Recall: \n",
    "\n",
    "$$\\text{sensitivity}=\\frac{\\text{TP}}{\\text{Real Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    "\n",
    "+ Specificity:\n",
    "\n",
    "$$\\text{specificity}=\\frac{\\text{TN}}{\\text{Real Negatives}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}$$\n",
    "\n",
    "Row-wise we find these two partial performance metrics:\n",
    "\n",
    "+ Precision or Positive Predictive Value:\n",
    "\n",
    "$$\\text{precision}=\\frac{\\text{TP}}{\\text{Predicted Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "+ Negative predictive value:\n",
    "\n",
    "$$\\text{NPV}=\\frac{\\text{TN}}{\\text{Predicted Negative}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FN}}$$\n",
    "\n",
    "The concept of positive and negative samples is purely arbitrary, thus we really have to remember the concepts of precision/positive predictive value and sensitivity/recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let us check the concepts with churn as the positive class\n",
    "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
    "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
    "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
    "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
    "\n",
    "print ('TP: ' + str(TP))\n",
    "print ('TN: ' + str(TN))\n",
    "print ('FP: ' + str(FP))\n",
    "print ('FN: ' + str(FN))\n",
    "print ('sensitivity/recall: '+ str(TP/(TP+FN)))\n",
    "print ('precision: '+ str(TP/(TP+FP)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>When to Use Recall and Precision</ins>\n",
    "\n",
    "In the context of classification tasks, understanding when to prioritize recall or precision can greatly impact the effectiveness of your model, especially in different real-world scenarios.\n",
    "\n",
    "#### Recall (Sensitivity)\n",
    "\n",
    "- **When to Use**: Recall should be prioritized when the cost of false negatives is high. In other words, when it's crucial to capture as many positive instances as possible.\n",
    "- **Example Scenario**: Medical diagnostics where failing to detect a disease (false negative) could be life-threatening. In such cases, it's better to have some false alarms (false positives) than to miss a positive case.\n",
    "  \n",
    "$$\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "#### Precision (Positive Predictive Value)\n",
    "\n",
    "- **When to Use**: Precision should be prioritized when the cost of false positives is high. This is important when the goal is to be as accurate as possible with the positive predictions.\n",
    "- **Example Scenario**: Email spam detection where it's more disruptive to wrongly classify important emails as spam (false positives) than to miss some spam emails (false negatives).\n",
    "\n",
    "$$\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
    "\n",
    "#### Specificity\n",
    "\n",
    "- **When to Use**: Specificity should be prioritized when it's important to capture true negatives. This metric is crucial when the presence of a condition is to be ruled out with certainty.\n",
    "- **Example Scenario**: Screening a rare disease in a large population, where it's important to identify those who definitely don't have the disease.\n",
    "\n",
    "$$\\text{specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$$\n",
    "\n",
    "#### Choosing the Right Metric\n",
    "\n",
    "Deciding whether to prioritize recall or precision (or specificity) often depends on the relative costs of false positives versus false negatives:\n",
    "\n",
    "- **High Cost of False Negatives**: Prioritize recall.\n",
    "- **High Cost of False Positives**: Prioritize precision.\n",
    "- **Balanced Approach Needed**: Consider using the F1-score, which is the harmonic mean of precision and recall, or the AUC-ROC curve, which considers both true positive rate (recall) and false positive rate (1 - specificity).\n",
    "\n",
    "Remember, the choice of metric should align with the business objectives or the specific requirements of the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the confusion matrix in our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict custormer churn, thus we may ask how often the classifier correctly predicts it. We will consider \"churn\" as the positive class. The question we are wondering about is the ratio between the $TP$ and all the $\\text{Real Positives}$. This is the *sensitivity* or *recall*. We are able to correctly predict $187/(187+296) = 0.39$ of the customers that cease the service. Observe that this value is consistent with the classification report when checking recall for class $1$.\n",
    "\n",
    "However, we have to trade-off this value with *precision*. Precision answers the question, from all the customers we predict will churn, which is the ratio of those that actually churn? This effectively tells us the price we are paying in terms of how many non-churn customers are being predicted as quitters. If we check this value, we can see it is $187/(187+59) = 76\\%$. This means that about 1 out 4 customers predicted as churn are not quitting the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nearest Neighbors**\n",
    "<p>\n",
    "<ul>\n",
    "<li> One of the simplest classifiers.\n",
    "<li> Smoothness of the model is governed by the number of the neighbors.\n",
    "<li> Hyper-parameter $k$ or $p$ in the $\\ell_p$ norm are set by cross-validation.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Decision Trees in Classification\n",
    "\n",
    "Decision trees are a popular machine learning technique used for both classification and regression tasks. They are especially known for their interpretability and ease of use. A decision tree operates by breaking down a dataset into smaller and smaller subsets based on different criteria, and decisions are made based on those subdivisions.\n",
    "\n",
    "### Basic Idea\n",
    "\n",
    "The core **idea** behind decision trees is the \"divide and conquer\" approach. This involves:\n",
    "\n",
    "1. **Partitioning the space**: The dataset's feature space is divided into distinct regions. For classification tasks, these regions are chosen to best separate the different classes.\n",
    "2. **Fitting a model in each patch**: After partitioning, a simple model (or decision) is applied in each region.\n",
    "\n",
    "In **classification trees**, each partition (or \"leaf\") represents a class, and all data points that fall into a region are assigned the corresponding class label.\n",
    "\n",
    "### Key Concepts in Decision Trees\n",
    "\n",
    "#### Gini Index\n",
    "\n",
    "- The Gini index is a metric used to measure how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Gini index can be represented as:\n",
    "\n",
    "  $$\\text{Gini}(D) = 1 - \\sum_{i=1}^m p_i^2$$\n",
    "\n",
    "  where $D$ is the dataset and $p_i$ is the proportion of the elements labeled with the current class.\n",
    "\n",
    "- **Usage**: During the construction of a tree, the Gini index is used to evaluate the partitions, and the decision tree algorithm will favor splits that result in regions with the lowest Gini index — that is, the most homogeneous groups.\n",
    "\n",
    "#### Entropy and Information Gain\n",
    "\n",
    "- **Entropy** is another measure used to determine how a space should be split. It quantifies the amount of uncertainty or randomness in the data. Entropy can be calculated as:\n",
    "\n",
    "  $$\\text{Entropy}(S) = - \\sum_{i=1}^m p_i \\log_2 p_i$$\n",
    "\n",
    "- **Information Gain** is the reduction in entropy after a dataset is split on an attribute. It is used to decide which feature to split on at each step in building the tree.\n",
    "\n",
    "#### Tree Pruning\n",
    "\n",
    "- To avoid overfitting, decision trees need to control their growth. **Pruning** is a technique that removes parts of the tree to make it simpler and prevent it from modeling noise in the training data.\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "- **Advantages**: Decision trees are easy to understand and interpret, can handle both numerical and categorical data, and require relatively little data preparation.\n",
    "- **Limitations**: They can create overly complex trees that do not generalize well from the training data (overfitting), and small variations in the data can result in different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Decision tree modeling\n",
    "\n",
    "Elements:\n",
    "\n",
    "- Splits using axis-orthogonal hyperplanes. This is the key that allows interpretability of the results.\n",
    "\n",
    "- At each internal node we test a value of a feature. A feature and a threshold are stored for each internal node.  \n",
    "\n",
    "- Leaves makes the class prediction. If leaves are pure, we have to store the class label. If leaves are impure, then the fraction of samples for each class is stored and its frequency is returned when queried.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our intuition on decision trees\n",
    "\n",
    "Let us build up our intuition with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Let's see what the boundary looks like in a toy problem.\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=10\n",
    "np.random.seed(2)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Tree\n",
    "import os\n",
    "dotfile = tree.export_graphviz(clf, out_file = \"toy_tree.dot\")\n",
    "\n",
    "os.system(\"dot -Tpng toy_tree.dot -o toy_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image\n",
    "Image(\"toy_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the meaning of the tree. The first node splits the training set using feature $1$ by applying the threshold $\\leq 3.04$. As a result we are able to correctly classify eleven of the thirty data points. Let us see the boundary in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0,max_depth=1)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second node splits the training set using feature $0$ by applying the threshold $\\leq 6.25$. Note that this only is used in the part of the space where feature $1$ is greater  than $3.04$. Observe that the remaining blue space is characterized by the following logical function: $(x_1>3.04) \\wedge (x_0\\leq 6.25)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is great about decision trees?\n",
    "\n",
    "+ Trees are easy for humans to interpret. It can be seen as a set of rules. Each path from root to one leaf of the tree is an AND combination of the thresholded features.\n",
    "+ Given a finite data set, decision trees can express any function of the input attributes. In ${\\bf R}^d$ we can isolate every point in the data set by constructing a box around each of them.\n",
    "+ There can be more than one tree that fits the same data. From all of them we would like a tree with minimum number of nodes. But the problem is NP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning the tree\n",
    "\n",
    "Because the problem is NP we can resort to a greedy construction algorithm. Greedy algorithms choose the current best binary partition without taking into account its impact on the quality of subsequent splits.\n",
    "\n",
    "The algorithm idea is as follows:\n",
    "\n",
    "+ Initialize the algorithm with a node associated to the full data set. \n",
    "\n",
    "**while** the list is not empty\n",
    "1. Retrieve the first node from the list.\n",
    "2. Find the data associated to that node.\n",
    "3. Find a splitting point.\n",
    "4. If the node is splittable, create the nodes linked to the parent node and put them in the exploration list.\n",
    "\n",
    "#### The splitting criterion\n",
    "\n",
    "There are many different splitting criteria. The most common ones are:\n",
    "\n",
    "+ Misclassification error\n",
    "+ Gini index\n",
    "+ Cross-entropy/Information gain/Mutual information\n",
    "\n",
    "Withouth going into details, misclassification error splits greedily select the split that corrects more data at each point. Gini index and cross-entropy probabilistically model the notion of impurity of a node. The split is chosen so that the average purity of the new nodes is maximized. Observe that as we descend in the tree the purity increases and eventually converge to pure leaves. A nice way of thinking about entropy is Pedro Domingos' simile with surprise. Entropy measures the average surprise/information a probabilistic result yields. In a binary variable, the maximum surprise occurs when both outcomes are equally probable, one has the maximum uncertainty on the result. Otherwise, the surprise decreases. This behavior is also display in Gini's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
    "gini = lambda p: 1. - (np.array(p)**2).sum()\n",
    "pvals = np.linspace(0, 1)        \n",
    "plt.plot(pvals, [entropy([p,1-p])/2. for p in pvals], label='Entropy')\n",
    "plt.plot(pvals, [gini([p,1-p]) for p in pvals], label='Gini')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees and overfitting\n",
    "\n",
    "Because trees are very expressive models they can model any training set perfectly and easily overfit.\n",
    "\n",
    "There are two ways of avoiding overfitting in trees:\n",
    "\n",
    "+ Stop growing the tree when the split is not statistically significant.\n",
    "+ Grow a full tree and post-prune.\n",
    "\n",
    "One of the simplest ways of post pruning is \"reduced error prunning\". It goes like this,\n",
    "\n",
    "1. Split data into training and validation\n",
    "2. Create a candidate tree on the training set\n",
    "3. Do until further pruning is harmful\n",
    "    1. Evaluate impact on the validation set of removing each posible node (with descendants)\n",
    "    2. Greedily remove the node that improves the performance the most.\n",
    "    \n",
    "Pruning is not implemented in sklearn at this moment. However let us check what happens in our customer churn prediction problem when we use a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "features = data[2]\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits()\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "    dt.fit(X_train,y_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the concepts with churn as the positive class\n",
    "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
    "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
    "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
    "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
    "\n",
    "print ('TP: ' + str(TP))\n",
    "print ('TN: ' + str(TN))\n",
    "print ('FP: ' + str(FP))\n",
    "print ('FN: ' + str(FN))\n",
    "print ('sensitivity/recall: '+ str(TP/(TP+FN)))\n",
    "print ('precision: '+ str(TP/(TP+FP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that by using a decision tree, the recall increased by $30\\%$ while having the precision at a simliar level than nearest neighbors. Let us check the first levels of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Let us check the the first three levels of the tree. GraphViz and PyDot are needed.\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "dt.fit(Xs,y)\n",
    "\n",
    "#Export Tree\n",
    "\n",
    "dotfile = tree.export_graphviz(dt, out_file = \"churn.dot\", feature_names = features)\n",
    "\n",
    "\n",
    "os.system(\"dot -Tpng churn.dot -o churn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image\n",
    "Image(\"churn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the first feature split and the values of the entropy according to the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
    "\n",
    "#Let us check the entropy on the root node\n",
    "#There are 2850 samples of customers that stay in the company. The frequency is\n",
    "proot = 2850/3333\n",
    "#And the entropy value is\n",
    "print( 'Root node entropy: '+ str(entropy([proot,1-proot])))\n",
    "\n",
    "#After the split we have the following frequencies for the left and right children\n",
    "pleft = np.sum([2476,166,13,111])/3122 #Frequency of label 0 on the left node\n",
    "pright = np.sum([32,5,44,3])/211 #Frequency of label 0 on the right node\n",
    "\n",
    "print( 'Left node entropy: '+ str(entropy([pleft,1-pleft])))\n",
    "print ('Right node entropy: '+ str(entropy([pright,1-pright])))\n",
    "\n",
    "#Information gain computes the difference between the entropy of the parent and the weighed entropies of the children\n",
    "# I = H_root - \\sum freq_i * H_i\n",
    "\n",
    "I = entropy([proot,1-proot]) - 3122/3333*entropy([pleft,1-pleft])+211/3333*entropy([pright,1-pright])\n",
    "\n",
    "print ('Information gain: '+ str(I))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split reduces the average entropy of the children, thus the splits are more pure.\n",
    "\n",
    "Observations:\n",
    "\n",
    "+ Observe that because we have restricted (for visualization purposes) the maximum depth of the tree, the leaves are not pure. \n",
    "+ Analyzing the leaves we can see that most of the clients that are hooked to the plan share the following conditions:\n",
    "\n",
    "$$(\\text{Day Charge} \\leq 1.5) \\wedge (\\text{Customer Service Calls} \\leq 1.47) \\wedge (\\text{International Plan} \\leq 1.36)$$\n",
    "\n",
    "Note that these values are preprocessed and as such they convey little interpretable information. Thus, in order to make sense of the former conditions we have to invert the transformation. We can do it in several ways, one way is to use interpolation and query for the feature of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "idx_global = features.tolist()\n",
    "\n",
    "#Day Charge\n",
    "plt.subplot(1,3,1)\n",
    "idx = idx_global.index('Day Charge')\n",
    "selection = X[:,idx]\n",
    "idx_sort = np.argsort(selection)\n",
    "plt.plot(Xs[idx_sort,idx], X[idx_sort,idx])\n",
    "spline = interpolate.UnivariateSpline(x=Xs[idx_sort,idx], y=X[idx_sort,idx])\n",
    "plt.plot(Xs[idx_sort,idx],spline(Xs[idx_sort,idx]),'r')\n",
    "plt.title('Day Charge transformation')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Transformed data')\n",
    "ax.set_ylabel('Original data')\n",
    "print ('Day Charge < '+str(spline([1.55])[0]))\n",
    "\n",
    "\n",
    "#CustServ Calls\n",
    "plt.subplot(1,3,2)\n",
    "idx = idx_global.index('CustServ Calls')\n",
    "selection = X[:,idx]\n",
    "idx_sort = np.argsort(selection)\n",
    "plt.plot(Xs[idx_sort,idx], X[idx_sort,idx],'o')\n",
    "spline = interpolate.UnivariateSpline(x=Xs[idx_sort,idx], y=X[idx_sort,idx])\n",
    "plt.plot(Xs[idx_sort,idx],spline(Xs[idx_sort,idx]),'r')\n",
    "plt.title('Customer Service Calls transformation')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Transformed data')\n",
    "ax.set_ylabel('Original data')\n",
    "print ('Customer service calls <' + str(spline([1.47])[0]))\n",
    "\n",
    "#International Plan\n",
    "plt.subplot(1,3,3)\n",
    "idx = idx_global.index(\"Int'l Plan\")\n",
    "selection = X[:,idx]\n",
    "idx_sort = np.argsort(selection)\n",
    "plt.plot(Xs[idx_sort,idx], X[idx_sort,idx],'o')\n",
    "spline = interpolate.UnivariateSpline(x=Xs[idx_sort,idx], y=X[idx_sort,idx])\n",
    "plt.plot(Xs[idx_sort,idx],spline(Xs[idx_sort,idx]),'r')\n",
    "plt.title('International Plan transformation')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Transformed data')\n",
    "ax.set_ylabel('Original data')\n",
    "print ('International Plan < ' + str(spline([1.36])[0]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the final profile for the 2476 customers that do not churn has in common:\n",
    "\n",
    "$$(\\text{Day Charge} \\leq 45) \\wedge (\\text{Customer Service Calls} < 4) \\wedge (\\text{International Plan}= \\text{NO})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Extending Support Vector Machines to the Non-Linear Case \n",
    "\n",
    "### A very brief introduction to kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that a linear model in the parameters can model non-linear boundaries provided we **explicitly** map original data non-linearly. For example, we can create a linear model with features squared. This will lead to a quadratic boundary with respect to the original space. There is another way of **implicitly** encoding non-linearities by means of **kernels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel encodes the notion of similarity between two data points. \n",
    "\n",
    "The change in the formulation involve the introduction of several concepts from mathematical analysis. For the sake of simplicity, we will skip the details (read \"The story of a kernel\" in the following paragraph or ask any detail if you are curious).\n",
    "\n",
    "As a result, any regularized cost function optimization problem such as SVM has a solution of following form,\n",
    "\n",
    "$$f(x) = \\sum\\limits_{i=1}^N \\alpha_i k(x_i,x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The story of a kernel**\n",
    "<p>\n",
    "*Disclaimer: This is  a highly mathematically non rigorous story of how kernels come into play in machine learning.*\n",
    "<p>\n",
    "At some point we talked about keeping complexity in check. For that purpose we need to measure complexity. And we saw that certain models, such as linear models can control it using the norm of weights. To the fact of adding this penalty to the objective function we are optimizing is called *regularization*.\n",
    "<p>\n",
    "However, it would be great to be able to measure the complexity of any function. To address this issue we have to resort to functional analysis. Functional analysis is a brach of mathematical analyisis that deals with spaces of functions. For that purpose a Hilbert space must be introduced so that similarity and distance among functions can be measured. A Hilbert space is a complete vector space with inner product. Intuitively, it generalizes the classical Euclidean space to infinite dimensions, and thus, to functional spaces.\n",
    "<p>\n",
    "One particular functional space is the Reproducing Kernel Hilbert Space (RKHS). In this space a function evaluated on a point $x$ is defined by the inner product of the function and the kernel evaluated on that point, i.e. $f(x) = \\langle f(\\cdot),K(x,\\cdot) \\rangle$, where $K$ is the kernel. This is called Riesz representation and it is the key for showing the most important result for our problems, *The Representer's theorem*.\n",
    "<p>\n",
    "The Respresenter's theorem states that the solution of any problem with the following form \n",
    "$$\n",
    "\tf^*=\\underset{f\\in \\mathcal{H}}{\\operatorname{arg\\,min}}\\frac{1}{n}\\sum_i{\\mathcal{L}(f(x_i),y_i)}+\\lambda\\|f\\|_{\\mathcal{H}}^2\n",
    "\t$$\n",
    "is given by\n",
    "$$f(x) = \\sum\\limits_{i=1}^N \\alpha_i k(x_i,x)$$\n",
    "where $x_i$ are our samples. \n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel has to be a positive semi-definite function, such as:\n",
    "\n",
    "+ Linear kernel: $$k(x_i,x_j) = x_i^Tx_j$$\n",
    "+ Polynomial kernel: $$k(x_i,x_j) = (1+ x_i^Tx_j)^p$$\n",
    "+ Radial Basis Function kernel $$k(x_i,x_j) = e^{-\\frac{\\|x_i-x_j\\|^2}{2\\sigma^2}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-error\"> On a practical side you can define a kernel by using your favorite distance $d(x_i,x_j)$ and defining the kernel as\n",
    "$$k(x_i,x_j) = e^{-\\gamma d(x_i,x_j)}, \\quad \\gamma>0$$\n",
    "where $\\gamma$ is a hyper-parameter that controls the decay of the exponential (we will tune it using cross-validation). Observe that RBF is an instantiation of this more general rule.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As comented before, kernels implicitly encode a non-linear transformation and remember that SVM finds the optimal hyperplane. By combining both concepts we have a linear method applied on a data on a transformed space, but we do not have to provide the explicit transformation.\n",
    "\n",
    "This becomes incredibly useful when one realizes that the feature mapping from a radial basis function kernel is a maping into a $\\infty$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le us build our intuition about how kernels work with the following video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('3liCbRZPrZA', size=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try it in a toy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the boundary looks like in a toy problem.\n",
    "%matplotlib inline\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=10\n",
    "np.random.seed(2)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "\n",
    "def SVC_gamma(gamma, C):\n",
    "    clf = svm.SVC(kernel = 'rbf', gamma = gamma, C = C)\n",
    "    clf.fit(X,y.ravel())\n",
    "    Z=clf.decision_function(data)\n",
    "    Z.shape=sz\n",
    "    plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "    plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "    plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "    plt.contour(XX,YY,Z,[0])\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)\n",
    "\n",
    "from IPython.html.widgets import interact    \n",
    "interact(SVC_gamma, gamma=(0.011,5.,0.01), C = (0.01,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 Application to customer churn prediction\n",
    "\n",
    "Let us apply the RBF kernel SVM to the customer churn prediction. Usually, discriminant classifiers are not affine invariant and we have to consider some feature normalization process. For the sake of fairness, we will use the same standarization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "features = data[2]\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5,  shuffle=False)\n",
    "kf.get_n_splits(X)\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    #Standard parameters\n",
    "    clf = svm.SVC(kernel='rbf', gamma = 0.051, C = 1)\n",
    "    clf.fit(X_train,y_train.ravel())\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us cross-validate the parameters and check if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "parameters = {'C':[ 2,4,8],'gamma':[0.02, 0.05, 0.1], 'class_weight':[{0:0.5},{0:1},{0:2}]}\n",
    "\n",
    "kf=model_selection.KFold( n_splits=5, shuffle=False)\n",
    "kf.get_n_splits(X)\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    #Standard parameters\n",
    "    clf = svm.SVC(kernel='rbf', class_weight={0:1,1:10})\n",
    "    # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "    clf = model_selection.GridSearchCV(clf, parameters, scoring='average_precision')\n",
    "    clf.fit(X_train,y_train.ravel())\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    #recall, f1, precision\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    print (str(clf.best_params_))\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** We are dealing with an unbalanced problem. Change the code to bias the model to deal with unbalancing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble learning\n",
    "\n",
    "When we want to purchase a product we usually read user's reviews. Before undergoing a major surjery procedure we seek the opinion of different experts. Ensemble learning mimicks one of the human uncertainty reduction mechanism, seeking additional opinions before making a major decision.\n",
    "\n",
    "Ensemble learning is divided in two steps:\n",
    "\n",
    "1. Train a set of classifiers\n",
    "2. Aggregate their results\n",
    "\n",
    "There are different reasons for using ensemble learning in practice:\n",
    "\n",
    "1. **Statistical reasons:** The combination of outputs of different classifiers may reduce the risk of an unfortunate selection of a poorly performing classifier.\n",
    "2. **Large scale data sets:** It makes little sense to only have one classifier on very large sets of data. Partition data in smaller subsets and aggregate seems like a good idea.\n",
    "3. **Divide and conquer:** Some problems too difficult for a single classifier to solve. The decision boundary may be too complex or lie outside the space of functions of the classifier.\n",
    "4. **Data fusion:** Different source fusion is usually a problem. One usually faces data coming from heterogeneous sources and the question is how to fuse these data. One solution is to train one classifier per source and the fuse the decision of those experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction to ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity\n",
    "\n",
    "One condition required for the system to work is that errors on different classifiers should be made on different samples in order for the strategic combination of the classifiers to correct possible errors in the judgement of the class o a particular instance. This effect has been called **diversity**.\n",
    "\n",
    "Diversity can be obtained in different ways:\n",
    "\n",
    "+ Using different training sets. Use resampling strategies to obtain different optimal classifiers. This effect is correlated with the notion of stability of the classifier and the concept of bias and variance of the classifier.\n",
    "+ Using different training parameters for different classifiers\n",
    "+ Combining different architectures. (i.e. svm, decission trees, ...)\n",
    "+ Training on different features. (i.e. random subspaces or random projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bagging and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Bootstrapping aggregation (Bagging) \n",
    "\n",
    "Bootstrapping means resampling the training data set with replacement. Usually the same number of data as the original data set is used.\n",
    "\n",
    "**Bootstrapping aggreagtion (aka. Bagging)** is a ensemble technique that uses multiple bootstrapped copies of the training set to build a set of classifiers. One classifier for each bootstrapped training copy. And then, use a combination technique, such as majority voting, in order to take the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check, how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us train an overfitted classifier. For example an SVC.\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=60\n",
    "np.random.seed(2)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "x = np.linspace(-5,15,200)\n",
    "XX,YY = np.meshgrid(x,x)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "mx= np.max(Z)\n",
    "mn= np.min(Z)\n",
    "Z.shape=sz\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "plt.colorbar()\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bagged_tree(X,y,C):\n",
    "    clf_list=[]\n",
    "    for i in range(C):\n",
    "        np.random.seed(None)\n",
    "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
    "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        Xr=X[idx,:]\n",
    "        yr=y[idx]\n",
    "        clf_list.append((clf.fit(Xr,yr.ravel()),idx))  #Add the indices for visualization purposes in test\n",
    "    return clf_list\n",
    "\n",
    "\n",
    "def visualize_bagged_tree(X,y,clf_list):\n",
    "    C = len(clf_list)\n",
    "    x = np.linspace(-5,15,200)\n",
    "    XX,YY = np.meshgrid(x,x)\n",
    "    sz=XX.shape\n",
    "    data=np.c_[XX.ravel(),YY.ravel()]\n",
    "    yhat=np.zeros((data.shape[0],len(clf_list)))\n",
    "    i=0\n",
    "    for dt,idx in clf_list:\n",
    "        yhat[:,i]=dt.predict(data)\n",
    "        Xr=X[idx,:]\n",
    "        yr=y[idx]\n",
    "        mx= np.max(yhat[:,i])\n",
    "        mn= np.min(yhat[:,i])\n",
    "        plt.subplot(int(np.floor(C/4))+1,4,i+1)\n",
    "        plt.scatter(Xr[(yr==1).ravel(),0],Xr[(yr==1).ravel(),1],color='r')\n",
    "        plt.scatter(Xr[(yr==-1).ravel(),0],Xr[(yr==-1).ravel(),1],color='b')\n",
    "        plt.imshow(yhat[:,i].reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "        plt.contour(XX,YY,yhat[:,i].reshape(sz),[0])\n",
    "        i=i+1\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20,7*int(np.floor(C/4))+1)\n",
    "    return yhat\n",
    "\n",
    "\n",
    "clf_list=train_bagged_tree(X,y,16)\n",
    "y_pred=visualize_bagged_tree(X,y,clf_list)\n",
    "y_pred = np.sum(y_pred,axis=1)\n",
    "\n",
    "print ('Process finnished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx= np.max(y_pred)\n",
    "mn= np.min(y_pred)\n",
    "plt.figure()\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(y_pred.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "plt.colorbar()\n",
    "plt.contour(XX,YY,y_pred.reshape(sz),[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zb = y_pred.reshape(sz)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(Zb, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "plt.contour(XX,YY,Zb,[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application to customer churn prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check this approach in the Churn problem. Recall that a single decision tree achieved an accuracy of $91.7\\%$, precision of $71\\%$ and recall of $72\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = 2*data[1]-1\n",
    "features = data[2]\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bagged_tree(X,y,C):\n",
    "    clf_list=[]\n",
    "    for i in range(C):\n",
    "        np.random.seed(None)\n",
    "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
    "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        Xr=X[idx,:]\n",
    "        yr=y[idx]\n",
    "        clf_list.append(clf.fit(Xr,yr.ravel()))  \n",
    "    return clf_list\n",
    "\n",
    "\n",
    "def test_bagged_tree(X,clf_list):\n",
    "    yhat=np.zeros((X.shape[0],len(clf_list)))\n",
    "    i=0\n",
    "    for dt in clf_list:\n",
    "        yhat[:,i]=dt.predict(X)\n",
    "        i=i+1\n",
    "    return np.sign(np.mean(yhat,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits(X)\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf_list = train_bagged_tree(X_train,y_train.ravel(),21)\n",
    "    yhat[test_index]=test_bagged_tree(X_test,clf_list) \n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm.T)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the solution accuracy increases by about $5\\%$, recall goes upt to $75\\%$ and precision also increases up to $91\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging** performance improvement is due to the reduction of the variance of the classifier while mantaining its bias.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest technique introduces a randomization over the feature selected for building  each tree in the ensemble in order to improve diversity in an attempt to reduce variance evan more. Let us code this variant of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "def train_random_forest(X,y,C,F):\n",
    "    F=int(np.ceil(np.sqrt(X.shape[1])))\n",
    "    clf_list=[]\n",
    "    for i in range(C):\n",
    "        np.random.seed(None)\n",
    "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
    "        feat_idx=np.random.permutation(np.arange(X.shape[1]))[:F]\n",
    "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        Xr=X[idx,:].copy()\n",
    "        Xr=Xr[:,feat_idx]\n",
    "        yr=y[idx]\n",
    "        clf_list.append((clf.fit(Xr,yr.ravel()),feat_idx))\n",
    "    return clf_list\n",
    "\n",
    "\n",
    "def test_random_forest(X,clf_list):\n",
    "    yhat=np.zeros((X.shape[0],len(clf_list)))\n",
    "    i=0\n",
    "    for dt,feat_idx in clf_list:\n",
    "        yhat[:,i]=dt.predict(X[:,feat_idx])\n",
    "        i=i+1\n",
    "    return np.sign(np.mean(yhat,axis=1)),yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = 2*data[1]-1\n",
    "print ('Labels: '+ str(np.unique(y)))\n",
    "features = data[2]\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "clf_list = train_random_forest(X,y,51,5)\n",
    "yhat,yk = test_random_forest(X,clf_list)\n",
    "acc = metrics.accuracy_score(yhat, y)\n",
    "print (yk.shape)\n",
    "print (np.sum(np.mean(yk,axis=1)>0))\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits(X)\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt = ensemble.RandomForestClassifier(n_estimators=51)\n",
    "    dt.fit(X_train,y_train)\n",
    "    yhat[test_index]=dt.predict(X_test)\n",
    "    #clf_list = train_random_forest(X_train,y_train,51,np.ceil(np.sqrt(X.shape[1])))\n",
    "    #yhat[test_index],yk = test_random_forest(X_test,clf_list)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print (acc)\n",
    "print (np.unique(yhat))\n",
    "print (np.unique(y_test))\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm.T)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Introduction to the multiclass problem and Error Correcting Output Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this moment we have applied several models in multi-class problems but we have barely talked about the problem of multiple classes in learning. \n",
    "\n",
    "First of all, there are very few models that intrinsically handle the multi-class case. By intrinsically handling the multi-class problem I am referring to methods in which we do not have to worry about how many classes our problem has. The two big families of models that can deal with the problem are\n",
    "\n",
    "+ Decision trees: the leaves encode the class.\n",
    "+ Nearest Neighbors: we only care about class labels of instances close to my query sample.\n",
    "\n",
    "What about the rest of the models? Did not Bayesian models or Neural Networks also handle this problem? Yes, they work in the multi-class case. But we have to worry about how many classes there are. In particular we have to build a model for each class and then take a maximum score/probability/confidence among the predictions. This way of addressing the multiclass problem is also known as **one-against-all** because we consider one model for each class while the samples from the rest of the classes are considered as negative samples. This is the first example of a reductionist framework.\n",
    "\n",
    "The reductionist framework refers to those ensemble methods that allows to reduce the multi-class problem to a set of binary problems. In a $K$ class problem, the two most common approaches in this framework are:\n",
    "\n",
    "+ **one-against-all:** We consider $K$ partitions of the problem, corresponding to setting one class as positive class and the rest as negative. \n",
    "+ **one-against-one:** We consider all posible pairs of classes and build a model for each subproblem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Create a multiclass toy problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "MAXN=5\n",
    "np.random.seed(0)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.25*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,-2]+1.25*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,3*np.ones((MAXN,1))])\n",
    "\n",
    "#Display data\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r',label='class 1')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b',label='class 2')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g',label='class 3')\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "#Train a LinearSVC in one-vs-all fashion\n",
    "clf_list=[]\n",
    "for i in range(3):\n",
    "    clf = svm.LinearSVC()\n",
    "    y_meta = y.copy()\n",
    "    #Create a binary problem with one class at +1 and the rest at -1\n",
    "    y_meta=np.where(y_meta == i+1 ,1,-1)\n",
    "    clf_list.append(clf.fit(X,y_meta.ravel()))\n",
    "\n",
    "#Test each classifier\n",
    "plt.figure()\n",
    "x = np.linspace(-5,15,200)\n",
    "XX,YY = np.meshgrid(x,x)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "i=1\n",
    "yhat_d=np.empty((data.shape[0],3))\n",
    "for c in clf_list:\n",
    "    yhat=c.predict(data)\n",
    "    #Visualization of each boundary\n",
    "    yhat_d[:,i-1]=c.decision_function(data)\n",
    "    mn = np.min(yhat)\n",
    "    mx = np.max(yhat)\n",
    "    plt.subplot(1,3,i)\n",
    "    plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='b',label='class 1')\n",
    "    plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='g',label='class 2')\n",
    "    plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r',label='class 3')\n",
    "    plt.imshow(yhat.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "    plt.contour(XX,YY,yhat.reshape(sz),[0])\n",
    "    i=i+1\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,9)\n",
    "\n",
    "y_final=np.argmax(yhat_d,axis=1)\n",
    "plt.figure()\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='b',label='class 1')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='g',label='class 2')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r',label='class 3')\n",
    "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "plt.contour(XX,YY,y_final.reshape(sz),[0, 1])\n",
    "plt.title(\"Final decision boundary\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Error correcting output coding\n",
    "\n",
    "Error correcting output coding is a generalization of the methods shown before. In the most general case each class is assigned a ternary code $c_i \\in \\{+1,0,-1\\}^l$ with length $l$. This step is called **coding**. In testing a new sample will be given a test code and this will be compared according to some distance to the class codewords. The class with the closest codeword will be selected as the predicted class. This step is called **decoding**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the coding step\n",
    "If we arrange the codewords as rows in a matrix we obtain the coding matrix $M \\in \\{+1,0,-1\\}^{K\\times l}$. Consider the following example with four classes and code length $l=3$:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>$h_1$</th>\n",
    "    <th>$h_2$</th>\n",
    "    <th>$h_3$</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$-1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The first class, $y_1$ is coded as $(1,1,1)$, the second $y_2$ is coded as $(1,-1,0)$, and so on.\n",
    "\n",
    "Note that the columns of the matrix define a binary problem involving all the classes in the following way: in same column, all classes with code $+1$ belongs to the same meta-class, all classes with code $-1$ to the other meta-class, and all classes with code $0$ are not considered in that particular problem. In our example, the first column defines a binary problem involving the discrimination of all samples from classes $y_1,y_2$ (coded as $+1$) against all the samples of $y_3,y_4$ (coded as $-1$). The second column only considers the samples of class $y_1$ against the samples of class $y_2$. Note that all the zero coded classes are not considered. \n",
    "\n",
    "Given a coding matrix a classifier is trained for each column according to the column defined binary problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Which are the coding matrix of one-vs-one and one-vs-all?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the decoding step\n",
    "\n",
    "Given a set of classifiers trained according to the problems defined by the columns of the coding matrix, in the prediction step all the classifiers are applyed to the testing sample. As a result a binary code $t$ is obtained. This coded is compared to all the class codes according to some decoding/distance metric. The most common ones are:\n",
    "\n",
    "+ Hamming decoding/$\\ell_1$-decoding\n",
    "$$d(a,b) = \\frac{1}{2}\\sum\\limits_{i=1}^l |a_i-b_i|$$\n",
    "\n",
    "+ Euclidean decoding\n",
    "$$d(a,b) = \\sqrt{\\sum\\limits_{i=1}^l (a_i-b_i)^2}$$\n",
    "\n",
    "For example, consider that $t=(-1,-1,-1)$. Note that there is no exact code in the coding matrix, thus we have to check for the closest one. If we apply Hamming decoding we obtain\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Hamming</th>\n",
    "    <th>Euclidean</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$3$</th>\n",
    "    <th>$\\sqrt{12}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$\\frac{3}{2}$</th>\n",
    "    <th>$\\sqrt{5}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$\\frac{3}{2}$</th>\n",
    "    <th>$\\sqrt{5}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$\\frac{1}{2}$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Observe that in both cases the sample will be predicted as class $y_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply this framework to the former problem using a one-vs-all approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Create a multiclass toy problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "MAXN=5\n",
    "np.random.seed(0)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.25*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,-2]+1.25*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([0*np.ones((MAXN,1)),1*np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,2*np.ones((MAXN,1))])\n",
    "x = np.linspace(-5,15,200)\n",
    "XX,YY = np.meshgrid(x,x)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "\n",
    "#Define the coding matrix\n",
    "M = np.array([[1, -1, -1],[-1, 1, -1],[-1, -1, 1]]) #1vsAll\n",
    "#M = np.array([[1, 1, 0 ],[-1, 0, 1],[0, -1, -1]]) #1vs1\n",
    "\n",
    "print ('Coding matrix M = \\n' + str(M))\n",
    "def inset(a,b): \n",
    "    return [item in b for item in a]\n",
    "\n",
    "def fit_ECOC(X, y, M):\n",
    "    clf_list=[]\n",
    "    for i in range(M.shape[1]): #For each column\n",
    "        y_meta=y.copy()\n",
    "        idx_c1 = np.where(inset(y, np.where(M[:,i]==1)[0]))[0]\n",
    "        idx_c2 = np.where(inset(y, np.where(M[:,i]==-1)[0]))[0]\n",
    "        clf = svm.LinearSVC()\n",
    "        clf_list.append(clf.fit(np.r_['0',X[idx_c1,:],X[idx_c2,:]],np.r_['0',np.ones((idx_c1.shape[0],1)),-np.ones((idx_c2.shape[0],1))].ravel()))\n",
    "    return clf_list\n",
    "\n",
    "def predict_ECOC(X,M, clf_list):\n",
    "    #Test codes\n",
    "    c = np.zeros((X.shape[0],M.shape[1]))\n",
    "    for i in range(M.shape[1]):\n",
    "        c[:,i]=clf_list[i].decision_function(X) #SOFT CODES\n",
    "        #c[:,i]=clf_list[i].predict(X) #HARD CODES \n",
    "    #Use Euclidean distance\n",
    "    i=0\n",
    "    d = np.zeros((X.shape[0],M.shape[0]))\n",
    "    for code in M:\n",
    "        d[:,i]=np.sum(np.power((c-code),2),axis=1)\n",
    "        i=i+1\n",
    "    return np.argmin(d,axis=1)    \n",
    "\n",
    "\n",
    "clf_list=fit_ECOC(X,y,M)\n",
    "y_final = predict_ECOC(data,M,clf_list)\n",
    "\n",
    "plt.scatter(X[(y==0).ravel(),0],X[(y==0).ravel(),1],color='b',label='class 1')\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='g',label='class 2')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r',label='class 3')\n",
    "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "plt.contour(XX,YY,y_final.reshape(sz),[0, 1])\n",
    "plt.title(\"Final decision boundary\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Replace the hard codes prediction by the soft coding by changing in the ECOC_predict() function the \".predict\" by \".decision_function\". Run the algorithm with one-against-all approach. What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Replace the coding matrix by one-vs-one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1, 1, 0 ],[-1, 0, 1],[0, -1, -1]])\n",
    "clf_list=fit_ECOC(X,y,M)\n",
    "y_final2 = predict_ECOC(data,M,clf_list)\n",
    "\n",
    "plt.scatter(X[(y==0).ravel(),0],X[(y==0).ravel(),1],color='b',label='class 1')\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='g',label='class 2')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r',label='class 3')\n",
    "plt.imshow(y_final2.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "\n",
    "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "plt.contour(XX,YY,y_final2.reshape(sz),[0, 1])\n",
    "plt.title(\"Final decision boundary\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "state": {
    "4bf645ae7e87465fa67a2ae08bdfa1cc": {
     "views": [
      {
       "cell_index": 75
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
