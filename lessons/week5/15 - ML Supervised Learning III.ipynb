{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning II: Introduction to Supervised Classification Methods\n",
    "\n",
    "CONCEPTS\n",
    "\n",
    "## PART 1: A practical approach to machine learning\n",
    "\n",
    "1. About the software.\n",
    "\n",
    "2. What is Machine Learning?\n",
    "\n",
    "3. Modeling the machine learning problem\n",
    "\n",
    "4. The supervised classification problem. A basic guided programatic example\n",
    "\n",
    "    4.1 Representing the problem in sklearn\n",
    "    \n",
    "    4.2 Learning and predicting\n",
    "    \n",
    "    4.3 More about the feature space\n",
    "    \n",
    "    4.4 Training and testing\n",
    "    \n",
    "    4.5 Model selection (I)\n",
    "\n",
    "## PART 2: Learning concepts and theory\n",
    "\n",
    "5. What is learning?\n",
    "\n",
    "    5.1 PAC-learning\n",
    "      \n",
    "6. Inside the learning model\n",
    "\n",
    "    6.4 The human machine learning algorithm\n",
    "    \n",
    "    6.5 Model class and hypothesis space\n",
    "    \n",
    "    6.6 Objective function\n",
    "    \n",
    "    6.7 Searching/Optimization/Learning algorithm\n",
    "    \n",
    "7. Learning curves and overfitting\n",
    "\n",
    "    7.1 Learning curves\n",
    "    \n",
    "    7.2 Overfitting\n",
    "        \n",
    "8. Cures to overfitting\n",
    "\n",
    "    8.1 Model selection (II)\n",
    "    \n",
    "    8.2 Regularization\n",
    "    \n",
    "    8.3 Ensemble\n",
    "    \n",
    "9. What to do when...?\n",
    "\n",
    "## PART 3: First models\n",
    "\n",
    "10. Generative and discriminative models\n",
    "    \n",
    "    10.1\tBayesian models (Naive Bayes) and some applications.\n",
    "    \n",
    "    10.2\tSupport Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: Models in Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generative and discriminative models.\n",
    "\n",
    "In the field of machine learning, we often encounter two fundamental approaches to understanding and modeling data: **Generative Models** and **Discriminative Models**. These models offer distinct perspectives on how to approach problems in machine learning, each with its own advantages and applications. This section aims to elucidate these concepts, providing a clearer understanding for students, particularly those for whom English is not a first language.\n",
    "\n",
    "- **Generative Models** are designed to capture how data is generated, allowing us to simulate and understand the underlying distribution of data. The primary objective of generative models is to estimate the joint probability distribution $P(x, y)$, where $x$ represents the input features and $y$ represents the output labels. By mastering this distribution, generative models enable us to perform tasks such as classification by computing the posterior probability $P(y | x)$, which indicates the probability of a label given the input data. This computation is facilitated by Bayes' Rule, which links the posterior probability with the likelihood $P(x | y)$ and the prior probability $P(y)$. A classic application of generative models is in scenarios where not only classification but also the generation of new, similar data points is required. For example, Naive Bayes classifiers, Hidden Markov Models, and Gaussian Mixture Models are all generative models that can predict labels and generate data that mimics the original dataset.\n",
    "\n",
    "\n",
    "- **Discriminative Models**, on the other hand, focus on directly learning the decision boundary or function that distinguishes between different classes in the dataset. Unlike generative models, discriminative models do not concern themselves with modeling the distribution of the data. Their goal is to predict the output label $y$ for a given input $x$ as accurately as possible, by approximating the conditional probability $P(y | x)$. This makes discriminative models particularly suited for tasks where the main objective is prediction, such as classification and regression. Examples of discriminative models include Logistic Regression, Support Vector Machines (SVM), and Neural Networks. These models are celebrated for their ability to produce accurate predictions, especially in complex scenarios where understanding the data's generation process is less critical.\n",
    "\n",
    "\n",
    "- An important subset of both generative and discriminative models is **Linear Models**. These models assume that the decision boundaries can be expressed as linear (or sometimes quadratic) functions of the input features. A linear model can be described by the equation $h(x) = a^T x + b$, where $a$ represents a vector of weights, $x$ is the vector of input features, and $b$ is a bias term. Despite their simplicity, linear models form the cornerstone of many machine learning algorithms because they are easy to interpret and can be the basis for more complex models. Through the exploration of simple linear examples of both generative and discriminative models, students can gain insights into how these models apply to real-world classification problems, offering a foundational understanding of linear decision boundaries.\n",
    "\n",
    "This comprehensive overview aims to clarify the distinctions and applications of generative and discriminative models in machine learning, with a special focus on making the concepts accessible to students for whom English is a second language. By providing examples and explaining the fundamental principles behind these models, the goal is to foster a deeper understanding of the approaches and their significance in the field of machine learning.\n",
    "\n",
    "[Short explanation](https://www.youtube.com/watch?v=XtYMRq7f7KA&ab_channel=VictorLavrenko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Bayesian models (Naive Bayes) and some applications\n",
    "\n",
    "Imagine we have a big basket of fruits, and we want to sort them into types like apples, bananas, and oranges. This task is similar to what the Naive Bayes classifier does in the world of machine learning, but instead of sorting fruits, it sorts information.\n",
    "\n",
    "#### What is Naive Bayes?\n",
    "\n",
    "Naive Bayes is like a smart sorting machine that helps us organize things based on what it knows about them. For our fruit example, think of Naive Bayes as a friend who helps you sort fruits by telling you which fruit is likely to be an apple, a banana, or an orange, based on their characteristics like color, shape, and taste.\n",
    "\n",
    "- **Bayes' Theorem**: This is the rule our sorting machine uses. It's a mathematical formula that helps predict the type of fruit based on its features. For instance, if you tell it a fruit is long and yellow, Bayes' Theorem helps it guess that it's probably a banana.\n",
    "\n",
    "- **Naive Assumption**: The \"naive\" part comes from the machine treating each feature (like color or shape) as if it's completely independent of the others. Even though this isn't how things work in real life (for example, oranges are both orange and round), this assumption makes our machine very fast and surprisingly good at guessing!\n",
    "\n",
    "#### How Does Naive Bayes Sort Fruits?\n",
    "\n",
    "Let's break it down into simple steps:\n",
    "\n",
    "1. **Learning**: First, you show the machine lots of different fruits and tell it what they are. This is like teaching it the game by showing it examples.\n",
    "\n",
    "2. **Guessing**: After learning, when you give the machine a new fruit it hasn't seen before, it uses what it learned to guess the fruit's type. It looks at the fruit's features (color, shape, taste) and calculates which type it's most likely to be.\n",
    "\n",
    "##### Seeing Naive Bayes in Action\n",
    "\n",
    "Imagine you draw circles around apples, bananas, and oranges based on their features. Naive Bayes does something similar in its mind. When a new fruit comes along, it sees where the fruit fits best based on the circles drawn from what it learned.\n",
    "\n",
    "#### Why is Naive Bayes So Special?\n",
    "\n",
    "Even though it makes simple assumptions, Naive Bayes can quickly sort through lots of information (or fruits) and make accurate guesses. It's like having a super-fast fruit sorter that gets better the more it learns.\n",
    "\n",
    "Next, we'll see how we can use this amazing sorter not just for fruits, but for sorting all kinds of things, making our lives easier and more organized. We'll take it step by step, ensuring it's fun and easy to understand.\n",
    "\n",
    "[Naive Bayes explained](https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to ensure a clean slate\n",
    "%reset -f\n",
    "\n",
    "# Enable plotting in the Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create synthetic data for two groups\n",
    "# The data for each group is generated around a different center point to simulate distinction\n",
    "X = np.concatenate([1.25*np.random.randn(40,2), 5+1.5*np.random.randn(40,2)]) \n",
    "y = np.concatenate([np.ones((40,1)), -np.ones((40,1))])\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "# Train the classifier with the synthetic data\n",
    "nb.fit(X, y.ravel())\n",
    "\n",
    "# Create a grid of points to visualize the decision boundary\n",
    "delta = 0.025\n",
    "xx = np.arange(-5.0, 10.0, delta)\n",
    "yy = np.arange(-5.0, 10.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "# Predict probabilities on the grid to visualize the decision boundary\n",
    "Z = nb.predict_proba(np.c_[XX.ravel(), YY.ravel()])\n",
    "Z = Z[:, 1].reshape(XX.shape)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "# Separate the data into two groups for plotting\n",
    "idxplus = y == 1\n",
    "idxminus = y == -1\n",
    "idxplus = idxplus.flatten()\n",
    "idxminus = idxminus.flatten()\n",
    "# Plot the first group in red\n",
    "plt.scatter(X[idxplus, 0], X[idxplus, 1], color='r', alpha=0.4)\n",
    "# Plot the second group in blue\n",
    "plt.scatter(X[idxminus, 0], X[idxminus, 1], color='b', alpha=0.4)\n",
    "# Overlay the probability density to visualize the decision boundary\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5, 10, -5, 10), alpha=0.3, vmin=0, vmax=1)\n",
    "# Draw the contour line where the probability is 0.5 (decision boundary)\n",
    "plt.contour(XX, YY, Z, [0.5])\n",
    "# Adjust the figure size for better visibility\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If the boundary is not linear why is it considered a linear model? It is not a linear model, though it is an affine model with respect to the weights. In the particular case of text classification we will use certain probability density functions that will make the model linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes classifier, a cornerstone of machine learning, particularly shines in its simplicity and effectiveness in classification tasks. Through the Python example provided, we've visualized not just the classification process but also how Naive Bayes makes decisions based on probabilities.\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "- **Simplicity in Action**: The Naive Bayes classifier, despite its underlying assumption of feature independence, demonstrates robust performance in classifying data into distinct groups. This example, using synthetic data, showcases how even with basic assumptions, significant insights can be gleaned.\n",
    "\n",
    "- **Visualization of Decision Boundaries**: By plotting the data points and the decision boundary, we gain a visual understanding of how Naive Bayes classifies data. The contour line at which the probability is 0.5 acts as the threshold—data points falling on one side of this line belong to one category, and those on the other side belong to another.\n",
    "\n",
    "- **Probabilistic Approach**: The background gradient in the visualization represents the probability of belonging to one of the categories. This gradient illustrates the probabilistic nature of Naive Bayes, offering more than just classifications—it provides insights into the certainty of its predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1 Basic document representation\n",
    "\n",
    "Imagine you're sorting through a pile of different stories, and you want to organize them into groups like \"adventure tales\" or \"fairy tales.\" This is what we call **text classification**, and we use a method called Naive Bayes to help us figure out which group each story belongs to.\n",
    "\n",
    "#### How Do We Understand Stories?\n",
    "\n",
    "To help Naive Bayes sort stories, we first need to translate the stories into a format it can understand. We do this using a **bag-of-words** approach. Think of this as taking each story and breaking it down into a list of words that we're interested in.\n",
    "\n",
    "- **Bag-of-Words Example**: Suppose we're interested in stories about \"pirates\" and \"wizards.\" We decide to focus on certain words like \"ship,\" \"treasure,\" \"wand,\" and \"spell.\" We then see how many times these words appear in each story.\n",
    "\n",
    "| Story | Ship | Treasure | Wand | Spell |\n",
    "|-------|------|----------|------|-------|\n",
    "| 1 (Pirates) | 3    | 2        | 0    | 0     |\n",
    "| 2 (Pirates) | 2    | 1        | 0    | 1     |\n",
    "| 3 (Wizards) | 0    | 0        | 2    | 3     |\n",
    "| 4 (Wizards) | 0    | 1        | 3    | 2     |\n",
    "\n",
    "- In this table, Story 1 has 3 \"ship\" words, 2 \"treasure\" words, and none of the \"wand\" or \"spell\" words, which helps Naive Bayes guess it's a pirate story.\n",
    "\n",
    "#### How Naive Bayes Makes Its Guess\n",
    "\n",
    "Naive Bayes looks at the word counts for each story. If a story has more \"pirate\" words like \"ship\" and \"treasure,\" it guesses it's about pirates. If it has more \"wizard\" words like \"wand\" and \"spell,\" it guesses it's about wizards.\n",
    "\n",
    "#### Key Points to Remember\n",
    "\n",
    "- **Counting vs. Presence**: Sometimes, just knowing whether a word appears in the story or not (yes/no) is enough, instead of counting how many times it appears.\n",
    "- **Word Order Doesn't Matter**: With bag-of-words, the order of words is ignored. \"The treasure is on the ship\" and \"The ship is on the treasure\" would be seen as the same. Although they mean different things, for our purpose of sorting stories, this simple approach can still be very effective.\n",
    "\n",
    "By transforming stories into bags of words, Naive Bayes assists us in categorizing them into themes like \"pirates\" or \"wizards.\" Despite its simplicity, this method is a powerful tool for helping computers understand and organize stories, much like we do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Naive Bayes Classifier in Document Classification\n",
    "\n",
    "Naive Bayes is a straightforward yet powerful approach used in predicting the category of documents. It's like playing a guessing game where, based on the words used in a document, we try to predict its topic, such as whether it's about \"economics\" or \"technology.\"\n",
    "\n",
    "#### How Naive Bayes Works\n",
    "\n",
    "At its heart, Naive Bayes selects the category that is most likely to be correct based on the words present in the document. This is done using a mathematical formula known as Bayes' Theorem:\n",
    "\n",
    "- To find out which category is the most probable, we use this formula:\n",
    "\n",
    "  $$\\hat{y} = \\arg\\max_y p(y|x).$$\n",
    "\n",
    "  Here, $\\hat{y}$ is our best guess for the category of the document, $y$ represents a possible category, and $x$ is the description of the document (like the words it contains).\n",
    "\n",
    "- Bayes' Theorem helps us in this guessing game by relating different probabilities:\n",
    "\n",
    "  $$p(y|x) = \\frac{p(x|y)p(y)}{p(x)}.$$\n",
    "\n",
    "  In simple terms, this tells us how likely a category is given the document's description. We calculate this by looking at how common the words are in each category ($p(x|y)$), how common each category is ($p(y)$), and how common these words are in all documents ($p(x)$).\n",
    "\n",
    "#### Simplifying the Process\n",
    "\n",
    "When we're classifying documents, we're essentially comparing which category is more likely based on the words used. For instance, if a document contains words more common in \"economics\" than in \"technology,\" it will be classified under \"economics.\"\n",
    "\n",
    "- Interestingly, we don't need to worry about $p(x)$, the probability of seeing a particular set of words, because it doesn't change our decision. This simplifies our formula to:\n",
    "\n",
    "  $$P(y|x) \\propto P(y)P(x|y)$$\n",
    "\n",
    "  This means we're mostly interested in how likely a category is ($P(y)$) and how likely we are to see these words if the document is in that category ($P(x|y)$).\n",
    "\n",
    "#### Naive Bayes' Special Assumption\n",
    "\n",
    "What makes Naive Bayes \"naive\" is its assumption that each word in the document affects the category independently of other words. This assumption allows us to simply multiply the probabilities of individual words relating to a category to find the overall likelihood:\n",
    "\n",
    "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y) = \\prod\\limits_{i=1}^N p(x_i|y)$$\n",
    "\n",
    "For example, the probability that a document is about \"technology\" given it contains certain words can be calculated by multiplying the probabilities of each of those words belonging to the \"technology\" category.\n",
    "\n",
    "#### Practical Application\n",
    "\n",
    "When applying Naive Bayes to document classification, we might not always know the prior probability of each category ($p(y)$). In such cases, we might treat all categories as equally likely or use what's known as a non-informative prior. This leads us to focus on the likelihood of words within categories, simplifying our task to finding the most likely category based on the words in the document.\n",
    "\n",
    "In summary, Naive Bayes helps us classify documents by comparing the probabilities of words within categories, making it a straightforward yet effective tool for understanding and organizing information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2 Estimating conditioned probabilities \n",
    "\n",
    "The last remaining step is the estimation of the individual conditional probabilities. There are two classical variants the **Multinomial Naive Bayes** and the **Bernoulli Naive Bayes**. The difference between both lies in the goal of what they are modeling. **In Multinomial NB we compute the probability of generating the observed document.** In this sense, we multiply the conditional probability of each word in the document for all words present in the document. An alternative view is the *Bernoulli model*. **In the Bernoulli Naive Bayes we compute the probability of the binary bag-of-words descriptor.** Observe that in the Bernouilli Naive Bayes the final probability depends on the words that appear in the document but also on the words that do not appear while in the multinomial NB it only depends on the words that appear. On the contrary, multinomial naive bayes takes into account the multiplicity of the words in the document while Bernoulli does not. Let us consider in this example the *Bernoulli model* that is consistent with our representation where a zero indicates a word is not present in the document and a one represents it is present. In order to estimate this probability we can use a frequentist approximation to probability, i.e. we will estimate the probability as the frequency of appearance of each term in each category. This computation divides the number documents where the word appears over the total number of documents. \n",
    "\n",
    "In our previous example, $p(x_3=1 (\\text{the word 'price' appears})|y =\\text{'tech'}) = 1/2$ and $p(x_3=1 (\\text{the word 'price' appears})|y =\\text{'eco'}) = 2/2$. This is computed by dividing the number of documents where the word price appear in a given category over the number of documents of that category.\n",
    "\n",
    "#### The zero probability effect\n",
    "In the former example the probability $p(x_5=1|y=\\text{'eco'}) = 0$. This implies that if the word 'mobile' appears the document can not belong to the class $\\text{'economy'}$. It is unreasonable to completely penalize a whole class by the appearance or not appearance of a single word. It is customary to assign to those cases a very low probability value instead. One well known approach to correct this effect is the so called **Laplace correction**. It is computed as follows,\n",
    "\n",
    "$$p(x_i=1 | y=c_k ) = \\frac{\\text{# of documents of class } c_k \\text{ where word } x_i \\text{ appears} + 1}{\\text{# of documents of class } c_k + M}$$\n",
    "\n",
    "where $M$ is the amount of words in the description. \n",
    "\n",
    "#### Underflow effect\n",
    "\n",
    "As the number of words in the description increase there is a higher probability that many of those words will not be present in the document. The product of many very small values may lead to floating point underflow effects. For this reason it is usual to use the log probability instead. This transformation does not change the decision boundary. In our simplified case\n",
    "\n",
    "$$\\log p(x|y) = \\sum\\limits_{i=1}^N \\log p(x_i|y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 Applying Naive Bayes to text classification\n",
    "\n",
    "In this example, our goal is to automatically categorize news according to their title into twenty-eight standard topics. In this problem we will deal with every New York Times front page story from 1996 to 2006, coded according to the Policy Agendas (http://www.policyagendas.org). This collection of data has been compiled by Amber E. Boydstun.\n",
    "\n",
    "Specifically, we are interested in classifying news from The New York Times in the following macro-topics according to its title:\n",
    "\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>\n",
    "1 \n",
    "<td>\n",
    "Macroeconomics\n",
    "<tr>\n",
    "<td>\n",
    "2 \n",
    "<td>\n",
    "Civil Rights, Minority Issues, and Civil Liberties \n",
    "<tr>\n",
    "<td>\n",
    "3\n",
    "<td>\n",
    "Health\n",
    "<tr>\n",
    "<td>\n",
    "4 \n",
    "<td>Agriculture\n",
    "<tr>\n",
    "<td>\n",
    "5 \n",
    "<td>Labor, Employment, and Immigration\n",
    "<tr>\n",
    "<td>\n",
    "6 \n",
    "<td> Education\n",
    "<tr>\n",
    "<td>\n",
    "7\n",
    "<td>Environment\n",
    "<tr>\n",
    "<td>\n",
    "8\n",
    "<td>Energy\n",
    "<tr>\n",
    "<td>\n",
    "10 \n",
    "<td>Transportation\n",
    "<tr>\n",
    "<td>\n",
    "12 \n",
    "<td>Law, Crime, and Family Issues\n",
    "<tr>\n",
    "<td>\n",
    "13 \n",
    "<td>Social Welfare\n",
    "<tr>\n",
    "<td>\n",
    "14 \n",
    "<td>Community Development and Housing Issues\n",
    "<tr>\n",
    "<td>\n",
    "15 \n",
    "<td>Banking, Finance, and Domestic Commerce\n",
    "<tr>\n",
    "<td>\n",
    "16 \n",
    "<td>Defense\n",
    "<tr>\n",
    "<td>\n",
    "17 \n",
    "<td>Space, Science, Technology and Communications\n",
    "<tr>\n",
    "<td>\n",
    "18 \n",
    "<td>Foreign Trade\n",
    "<tr>\n",
    "<td>\n",
    "19 \n",
    "<td>International Affairs and Foreign Aid\n",
    "<tr>\n",
    "<td>\n",
    "20 \n",
    "<td>Government Operations\n",
    "<tr>\n",
    "<td>\n",
    "21 \n",
    "<td>Public Lands and Water Management\n",
    "<tr>\n",
    "<td>\n",
    "24 \n",
    "<td>State and Local Government Administration\n",
    "<tr>\n",
    "<td>\n",
    "26 \n",
    "<td>Weather and Natural Disasters\n",
    "<tr>\n",
    "<td>\n",
    "27 \n",
    "<td>Fires\n",
    "<tr>\n",
    "<td>\n",
    "28 \n",
    "<td>Arts and Entertainment\n",
    "<tr>\n",
    "<td>\n",
    "29 \n",
    "<td>Sports and Recreation\n",
    "<tr>\n",
    "<td>\n",
    "30 \n",
    "<td>Death Notices\n",
    "<tr>\n",
    "<td>\n",
    "31 \n",
    "<td>Churches and Religion\n",
    "<tr>\n",
    "<td>\n",
    "99 \n",
    "<td>Other, Miscellaneous, and Human Interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#load data\n",
    "import pandas as pd\n",
    "data=pd.read_csv('./datasets/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the data set in two set: \n",
    "    \n",
    "+ We will train the classifier with news up to 2004.\n",
    "+ We will test the classifier in news from 2005 and 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Splitting the dataset based on date: Training data will be from before 1/1/2004, and testing data from 2004 to 2006\n",
    "split = pd.to_datetime(pd.Series(data['Date'])) < pd.datetime(2004, 1, 1)\n",
    "\n",
    "# Extracting the 'Title' column as our raw data\n",
    "raw_data = data['Title']\n",
    "\n",
    "# Splitting the titles into training and testing sets based on the date\n",
    "raw_train = raw_data[split]\n",
    "raw_test = raw_data[np.logical_not(split)]\n",
    "\n",
    "# Extracting the topic labels for our dataset\n",
    "y = data['Topic_2digit']\n",
    "\n",
    "# Splitting the labels into training and testing sets corresponding to our title splits\n",
    "y_train = y[split]\n",
    "y_test = y[np.logical_not(split)]\n",
    "\n",
    "# Printing out the sizes of our training and testing datasets to ensure the split was done correctly\n",
    "print('Check the split sizes, train, test and total amount of data:')\n",
    "print(raw_train.shape, raw_test.shape, raw_data.shape)\n",
    "\n",
    "# Displaying the unique labels in our dataset to understand the classification categories\n",
    "print('Display the labels:')\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary tool for text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initializing the CountVectorizer with specific parameters\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=2,  # A word must appear in at least two documents to be considered\n",
    "    stop_words='english',  # Removing common English words (e.g., 'and', 'the', 'of') that don't contribute much to the meaning\n",
    "    strip_accents='unicode'  # Removing accents from characters for consistency\n",
    ")\n",
    "\n",
    "# Demonstrating the preprocessing and tokenization process with an example\n",
    "test_string = raw_train[0]  # Taking the first title from the training set as an example\n",
    "print(\"Example: \" + test_string + \"\\n\")\n",
    "# Showing the result of preprocessing (e.g., lowercasing, removing punctuation)\n",
    "print(\"Preprocessed: \" + vectorizer.build_preprocessor()(test_string) + \"\\n\")\n",
    "# Displaying the list of words (tokens) after splitting the preprocessed text\n",
    "print(\"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string)) + \"\\n\")\n",
    "# Applying the full analyzer (preprocessing, tokenizing, and filtering stop words)\n",
    "print(\"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string)) + \"\\n\")\n",
    "\n",
    "# Processing the entire datasets to convert the raw text into a matrix of token counts\n",
    "X_train = vectorizer.fit_transform(raw_train)  # Learning the vocabulary and transforming the training set\n",
    "X_test = vectorizer.transform(raw_test)  # Transforming the test set based on the learned vocabulary\n",
    "\n",
    "# Printing the total number of tokens (unique words) found in the dataset\n",
    "print(\"Number of tokens: \" + str(len(vectorizer.get_feature_names())) + \"\\n\")\n",
    "# Displaying a slice of the tokens for inspection\n",
    "print(\"Extract of tokens:\")\n",
    "print(vectorizer.get_feature_names()[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable plotting directly within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the Bernoulli Naive Bayes classifier from scikit-learn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Initialize the classifier\n",
    "nb = BernoulliNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_hat = nb.predict(X_test)\n",
    "\n",
    "# Import necessary tools for evaluation\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest', cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9, 9)\n",
    "\n",
    "# Print the classification accuracy: the proportion of correctly predicted instances\n",
    "print(\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "\n",
    "# Call the function to plot the confusion matrix for the test predictions\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "\n",
    "# Print a detailed classification report showing precision, recall, f1-score, and support for each class\n",
    "print(\"Classification Report:\")\n",
    "print(metrics.classification_report(y_hat, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Identify the three most simple classes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data for future use.\n",
    "import pickle\n",
    "ofname = open('NYT_data.pkl', 'wb')\n",
    "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the top N most predictive features per class?\n",
    "N = 5\n",
    "voc = vectorizer.get_feature_names()\n",
    "for i, label in enumerate(np.unique(y)):\n",
    "    topN = np.argsort(nb.coef_[i])[-N:]\n",
    "    print ('Code: '+ str(label) + ' Terms : '+ str([voc[i] for i in topN]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check what would happen if we enrich the data set with the summary of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data['Title']+data['Summary']\n",
    "raw_train = raw_data[split]\n",
    "raw_test = raw_data[np.logical_not(split)]\n",
    "y = data['Topic_2digit']\n",
    "y_train = y[split]\n",
    "y_test = y[np.logical_not(split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us tokenize the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, \n",
    " stop_words='english', \n",
    " strip_accents='unicode')\n",
    "\n",
    "#example\n",
    "test_string = raw_train[0]\n",
    "print (\"Example: \" + test_string +\"\\n\")\n",
    "print (\"Preprocessed: \" + vectorizer.build_preprocessor()(test_string)+\"\\n\")\n",
    "print (\"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string))+\"\\n\")\n",
    "print (\"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string))+\"\\n\")\n",
    "\n",
    "\n",
    "#Fit and convert data\n",
    "X_train = vectorizer.fit_transform(raw_train)\n",
    "X_test = vectorizer.transform(raw_test)\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"Number of tokens: \" + str(len(vectorizer.get_feature_names())) +\"\\n\")\n",
    "print (\"Extract of tokes:\")\n",
    "print( vectorizer.get_feature_names()[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "y_hat = nb.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data for future use.\n",
    "import pickle\n",
    "ofname = open('NYT_context_data.pkl', 'wb')\n",
    "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the top N most predictive features per class?\n",
    "N = 5\n",
    "voc = vectorizer.get_feature_names()\n",
    "for i, label in enumerate(np.unique(y)):\n",
    "    topN = np.argsort(nb.coef_[i])[-N:]\n",
    "    print ('Code: '+ str(label) + ' Terms : '+ str([voc[i] for i in topN]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that adding the small summary improves the recognition rate by $10\\%$. \n",
    "\n",
    "As a side note, Naive Bayes with these models creates a linear decision boundary. For this reason, sometimes NB is called a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a powerful method used in machine learning for classification tasks. They belong to a category of algorithms known as discriminative learning, where the goal is to find a decision boundary that separates different classes in the data.\n",
    "\n",
    "#### What Makes SVM Special?\n",
    "\n",
    "Unlike other linear models such as the perceptron or logistic regression, SVM offers a more robust approach to classification. Here's why:\n",
    "\n",
    "- **Explicit Boundary Modeling**: SVM focuses on finding the best possible boundary (or hyperplane in higher dimensions) that separates the classes. This boundary is chosen not just to separate the classes but to do so in a way that maximizes the margin between the closest points of the classes to the boundary. These closest points are known as support vectors, giving the algorithm its name.\n",
    "\n",
    "- **Versatility**: While the classical model for SVM is linear, it can be extended to handle non-linear classification using something called the kernel trick. This allows SVM to classify data that isn't linearly separable by transforming it into a higher-dimensional space where a linear separator does exist.\n",
    "\n",
    "#### The Intuition Behind SVM\n",
    "\n",
    "Imagine you're trying to draw a line that separates apples from oranges on a table. SVM aims to draw this line not just anywhere but in such a way that the smallest distance from the line to the nearest apple or orange is maximized. This ensures that the decision boundary is as far away from the closest points of each class as possible, providing a buffer that helps make the classification more robust to new data points.\n",
    "\n",
    "- **Maximizing the Margin**: The key idea is to find the widest possible \"street\" (margin) between the classes, with the \"edges\" of this street just touching the nearest points of each class. These points at the edge are the support vectors.\n",
    "\n",
    "- **Handling More Complex Data**: For data that can't be separated by a straight line, SVM uses the kernel trick to project the data into a higher-dimensional space where a linear separator is possible. This is like lifting the apples and oranges off the table into the air to find a plane that separates them.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "SVM is a foundational tool in machine learning, known for its ability to create clear, well-defined boundaries between classes. Its ability to handle both linear and non-linear data makes it a versatile choice for a wide range of classification problems. Understanding the principles behind SVM allows for deeper insights into how machines can learn to distinguish between different categories of data, making it a crucial part of any machine learning toolkit.\n",
    "\n",
    "\n",
    "[Link SVM](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.html.widgets import interact\n",
    "\n",
    "class HLA():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        self.X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
    "        self.y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
    "        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n",
    "        plt.scatter(self.X[40:,0],self.X[40:,1],color='b') \n",
    "        delta = 0.025\n",
    "        xx = np.arange(-5.0, 10.0, delta)\n",
    "        yy = np.arange(-5.0, 10.0, delta)\n",
    "        XX, YY = np.meshgrid(xx, yy)\n",
    "        Xf = XX.flatten()\n",
    "        Yf = YY.flatten()\n",
    "        self.sz=XX.shape\n",
    "        self.data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "    def run(self,w0,w1,offset):\n",
    "        w=np.array([w0,w1])\n",
    "        w.shape=(2,1)\n",
    "        Z = self.data.dot(w)+offset\n",
    "        Z.shape=self.sz\n",
    "        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n",
    "        plt.scatter(self.X[40:,0],self.X[40:,1],color='b')\n",
    "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-30, vmax=30)\n",
    "        XX = self.data[:,0].reshape(self.sz)\n",
    "        YY = self.data[:,1].reshape(self.sz)\n",
    "        plt.contour(XX,YY,Z,[0])\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(9,9)\n",
    "   \n",
    "\n",
    "def decorator(w0,w1,offset):\n",
    "    widget_hla.run(w0,w1,offset)\n",
    "    \n",
    "widget_hla = HLA()\n",
    "interact(decorator, w0=(-10.,10.), w1=(-10.,10.), offset=(-20.,40.));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Using the former widget, check manually the following configurations:\n",
    "\n",
    "<li> $(w_0,w_1,\\text{offset}) = (-1.7, -3.1, 10)$\n",
    "<li> $(w_0,w_1,\\text{offset}) = (-3.7, -0.5, 10.3)$\n",
    "<li> $(w_0,w_1,\\text{offset}) = (-7.5, -3.2, 28.8)$\n",
    "<p>\n",
    "Which one of those configuration do you think yields a better boundary? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**INTUITION:** The Support Vector Machine classifer finds the boundary with maximum distance/**margin** to both classes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- It implicitly models the notion of noise. One expects that the boundary with maximum margin will be robust to small perturbations in the data.\n",
    "- A maximum margin classifier has a unique solution in the separable case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the result of fitting a SVM classifier using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the global namespace in the notebook to start fresh\n",
    "%reset -f\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm  # Import SVM model from scikit-learn\n",
    "\n",
    "# Define a class for the SVM example\n",
    "class svm_example():\n",
    "    def __init__(self):\n",
    "        '''Data creation: Generates synthetic data for classification'''\n",
    "        np.random.seed(1)  # Set seed for reproducibility\n",
    "        # Generate synthetic data: two clusters with normal distribution\n",
    "        self.X = np.concatenate([1.25*np.random.randn(40,2), 5+1.5*np.random.randn(40,2)])\n",
    "        # Generate labels: first 40 are 1, next 40 are -1\n",
    "        self.y = np.concatenate([np.ones((40,1)), -np.ones((40,1))])\n",
    "\n",
    "    def run(self):\n",
    "        '''Fit a linear SVM: Train an SVM classifier with a linear kernel'''\n",
    "        self.clf = svm.SVC(kernel='linear')  # Initialize the SVM with a linear kernel\n",
    "        self.clf.fit(self.X, self.y.ravel())  # Fit the SVM model with the data\n",
    "        \n",
    "    def display(self):\n",
    "        '''Display the decision boundary, margins, and support vectors'''\n",
    "        # Create a mesh grid for plotting decision boundary\n",
    "        delta = 0.25\n",
    "        xx = np.arange(-5.0, 10.0, delta)\n",
    "        yy = np.arange(-5.0, 10.0, delta)\n",
    "        XX, YY = np.meshgrid(xx, yy)\n",
    "        \n",
    "        # Prepare the grid points for prediction\n",
    "        Xf = XX.flatten()\n",
    "        Yf = YY.flatten()\n",
    "        sz = XX.shape\n",
    "        data = np.concatenate([Xf[:, np.newaxis], Yf[:, np.newaxis]], axis=1)\n",
    "        \n",
    "        # Predict the decision function value for each grid point\n",
    "        Z = self.clf.decision_function(data)\n",
    "        Z.shape = sz\n",
    "        \n",
    "        # Plot the data points: red for one class, blue for the other\n",
    "        plt.scatter(self.X[0:40, 0], self.X[0:40, 1], color='r')\n",
    "        plt.scatter(self.X[40:, 0], self.X[40:, 1], color='b')\n",
    "        \n",
    "        # Display the decision boundary and margins\n",
    "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10), alpha=0.3, vmin=-3, vmax=3)\n",
    "        plt.contour(XX, YY, Z, [-1, 0, 1], colors=['b', 'k', 'r'])  # Draw the margins and decision boundary\n",
    "        \n",
    "        # Enhance the plot\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(9, 9)\n",
    "        \n",
    "        # Print the number of support vectors for each class\n",
    "        print('Number of support vectors: ' + str(np.sum(self.clf.n_support_)))\n",
    "        \n",
    "        # Highlight the support vectors on the plot\n",
    "        plt.scatter(self.clf.support_vectors_[:, 0], \n",
    "                    self.clf.support_vectors_[:, 1], \n",
    "                    s=120, \n",
    "                    facecolors='none', \n",
    "                    linewidths=2,\n",
    "                    zorder=10)\n",
    "        \n",
    "        # Print the coefficients of the decision function (w0, w1) and the offset\n",
    "        print('(w0,w1) = ' + str(10*self.clf.coef_[0]))\n",
    "        print('offset = ' + str(10*self.clf.intercept_[0]))\n",
    "        \n",
    "        # Return grid for further use if necessary\n",
    "        return XX, YY, Z\n",
    "\n",
    "# Create an instance of the svm_example class\n",
    "c = svm_example()\n",
    "c.run()  # Train the SVM model\n",
    "XX, YY, Z = c.display()  # Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there is a critical subset of data points. These are called **Support Vectors**. If any of those points disappear the boundary changes.  The decision boundary depends on the support vectors, thus we have to store them in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS: **\n",
    "<li> Set the azimuth to $113$ and elevation to $0$. Observe the data points and the relative position of the hyperplane. \n",
    "<li> Change the elevation to $90$. Describe this projection.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.1 Modeling the Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geometry of the hyperplane\n",
    "A hyperplane in ${\\bf R}^d$ is defined as an affine combination of the variables: $\\pi\\equiv a^Tx + b = 0$. \n",
    "\n",
    "Features:\n",
    "\n",
    "+ A hyperplane splits the space in two half-spaces. The evaluation of the equation of the hyperplane on any element of one of the half-space is a positive value. It is a negative value for all the elements in the other half-space.\n",
    "+ The distance of a point $x \\in{\\bf R}^d$ to the hyperplane $\\pi$ is \n",
    "$$d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling the separating hyperplane\n",
    "Given a binary classification problem with training data $\\mathcal{D}=\\{(x_i,y_i)\\},\\; i=1\\dots N, \\; y_i\\in\\{+1,-1\\} $. Consider $\\mathcal{S} \\subseteq \\mathcal{D}$ the subset of all data points belonging to class $+1$, $\\mathcal{S}=\\{x_i | y_i=+1\\}$, and $\\mathcal{R}=\\{x_i | y_i=-1\\}$ its complement. \n",
    "\n",
    "Then the problem of finding a separating hyperplane consists of fulfilling the following constraints\n",
    "\n",
    "$$a^Ts_i+b>0\\; \\text{and}\\; a^Tr_i+b<0 \\quad \\forall s_i\\in\\mathcal{S}, r_i\\in\\mathcal{R}.$$\n",
    "\n",
    "Note the strict inequalities in the formulation. Informally, we can consider the smallest satisfied constraint. And observe that the rest must be satisfied with a larger value. Thus, we can arbitrarily set that value to 1 and rewrite the problem as $$a^Ts_i+b\\geq 1\\; \\text{and}\\; a^Tr_i+b\\leq -1.$$\n",
    "\n",
    "This is a *feasibility problem* and it is usually written in the following way in optimization standard notation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & 1\\\\\n",
    "\\text{subject to} & a^T r_i + b \\leq -1,\\; \\forall r_i \\in \\mathcal{R}\\\\\n",
    "& a^T s_i + b \\geq 1\\; \\forall s_i \\in \\mathcal{S}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or in a compact way\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & 1\\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The solution of this problem is not unique, e.g. remember all the parameters of the 'Human Learning Algorithm'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The maximum margin hyperplane\n",
    "\n",
    "Selecting the maximum margin hyperplane requires to add a new constraint to our problem. Remember from the geometry of the hyperplane that the distance of any point to a hyperplane is given by $d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$. \n",
    "\n",
    "Recall that we want positive data to be beyond value 1 and negative data below -1. Thus, what is the distance value we want to maximize?\n",
    "\n",
    "The positive point closest to the boundary is at $1/\\|a\\|_2$ and the negative point closest to the boundary data point is also at $1/\\|a\\|_2$. Thus data points from different classes are at least $2/\\|a\\|_2$ apart. \n",
    "\n",
    "Recall that our goal is to find the separating hyperplane with maximum margin, i.e. with maximum distance among elements from different classes. Thus, we can complete the former formulation with our last requirement as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{maximize} & 2/\\|a\\|_2 \\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\|a\\|_2/2 \\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This formulation has a solution as long as the problem is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with the non-separable case\n",
    "\n",
    "In order to deal with misclassifications, we are going to introduce a new set of variables $\\xi_i$, that represents the amount of violation in the $i-th$ constraint. If the constraint is already satisfied, then $\\xi_i=0$, and $\\xi_i>0$ otherwise. Because $\\xi_i$ is related to the errors, we would like to keep this amount as close so zero as possible. This makes us introduce a element in the objective trading-off with the maximum margin.\n",
    "\n",
    "The new model becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\|a\\|_2/2 + C \\sum\\limits_{i=1}^N \\xi_i\\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1 - \\xi_i,\\; i=1\\dots N\\\\\n",
    "& \\xi_i\\geq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $C$ is the trade-off parameter that roughly balances margin and misclassification rate. This formulation is also called **soft-margin SVM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take home ideas:**\n",
    "<ul>\n",
    "<li> Classical SVM fits a hyperplane separating boundary. </li>\n",
    "<li> The hyperplane is defined to achieve the maximum margin. </li>\n",
    "<li> If the problem is not linearly separable a new term related to the misclassification performance is introduced that trades-off with the margin. This trade-off is governed by parameter $C$ (or $\\nu$ in $\\nu$-SVM). </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The New York Times problem again\n",
    "\n",
    "Let us now apply our knowledge to the New York Times headlines topic prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover NTY data\n",
    "import pickle\n",
    "fname = open('NYT_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "X_test = data[2]\n",
    "y_test = data[3]\n",
    "print ('Loading ok.')\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_hat = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover NTY data\n",
    "import pickle\n",
    "fname = open('NYT_context_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "X_test = data[2]\n",
    "y_test = data[3]\n",
    "print ('Loading ok.')\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_hat = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default parameters we can improve the recognition rate by $10\\%$. However we can not check the most important words. Can we find a better trade-off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for model selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "\n",
    "# Defining the parameter grid: \n",
    "# 'C' is a regularization parameter for LinearSVC. It controls the trade off between achieving a low training error and a \n",
    "# low testing error (generalization).\n",
    "# A smaller 'C' value leads to a smoother decision boundary (less fitting to the training data), \n",
    "# while a larger 'C' encourages the model to classify all training examples correctly by giving the model more flexibility.\n",
    "# Here, we're defining a range of 'C' values to try out with GridSearchCV to find the best one.\n",
    "parameters = {'C': [0.01, 0.05, 0.1, 0.5, 1, 10]}\n",
    "\n",
    "# Initializing the LinearSVC model\n",
    "svc = svm.LinearSVC()\n",
    "\n",
    "# Setting up GridSearchCV:\n",
    "# 'svc' is the SVM model with a linear kernel to be trained.\n",
    "# 'parameters' contains the grid of parameters ('C' values here) we want to try out.\n",
    "# GridSearchCV will systematically work through the combinations of parameters (different 'C' values),\n",
    "# train the model for each combination, and evaluate its performance.\n",
    "clf = model_selection.GridSearchCV(svc, parameters)\n",
    "\n",
    "# Fitting GridSearchCV:\n",
    "# This will train the LinearSVC model multiple times with the different 'C' values specified in 'parameters'.\n",
    "# For each 'C' value, it uses cross-validation to evaluate the model's performance.\n",
    "# Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to\n",
    "# an independent data set.\n",
    "# It does this by partitioning the original training data set into a training set to train the model,\n",
    "# and a validation set to evaluate it. This process is repeated for each 'C' value.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# After .fit() completes, clf (our GridSearchCV object) will contain a lot of information:\n",
    "# - The best 'C' value found.\n",
    "# - The model fitted with the best 'C' value.\n",
    "# - The scores or performance metrics for each 'C' value tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print ('The best parameterization is ' + str(clf.best_params_))\n",
    "print ('The achieved score is ' + str(clf.best_score_))\n",
    "\n",
    "print ('Checking the rest of the scores \\n')\n",
    "import matplotlib.pyplot as plt\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "\n",
    "plt.plot(clf.cv_results_['mean_test_score'],'r',marker='o')\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels([0.01, 0.05, 0.1, 0.5, 1, 10])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Best Parametrization and Achieved Score in GridSearchCV\n",
    "\n",
    "When performing a grid search in machine learning using `GridSearchCV`, the goal is to find the best parameters for a model that result in the highest performance score. Here's what these terms mean:\n",
    "\n",
    "### Best Parametrization\n",
    "\n",
    "- **Best Parametrization**: This refers to the set of parameters that gave the best results after the grid search has been completed. In the context of an SVM with a `C` parameter, it indicates the value of `C` that led to the best model performance. For example, if `GridSearchCV` returns `{'C': 1}`, it means that using `C=1` for the SVM resulted in the most accurate predictions during the cross-validation process.\n",
    "\n",
    "### Achieved Score\n",
    "\n",
    "- **Achieved Score**: After finding the best parametrization, `GridSearchCV` also provides the best score that was achieved with this parameter. This score is a number that reflects how well the model with the best parameters is performing. The specific meaning of the score depends on the scoring method used (e.g., accuracy, precision, recall, F1 score). For instance, if the score is `0.60212765`, and we are using accuracy as our scoring metric, it indicates that the model with the best parameter (`C=1`) was able to correctly predict the class labels for approximately 60.21% of the cross-validated dataset.\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "The process of using `GridSearchCV` not only helps in tuning the model to find the best settings but also gives us an estimate of how well the model is likely to perform on unseen data. It's important to look at both the best parameters and the achieved score to understand the potential effectiveness of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
