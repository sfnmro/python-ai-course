{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning II: Introduction to Supervised Classification Methods\n",
    "\n",
    "CONCEPTS\n",
    "\n",
    "## PART 1: A practical approach to machine learning\n",
    "\n",
    "1. About the software.\n",
    "\n",
    "2. What is Machine Learning?\n",
    "\n",
    "3. Modeling the machine learning problem\n",
    "\n",
    "4. The supervised classification problem. A basic guided programatic example\n",
    "\n",
    "    4.1 Representing the problem in sklearn\n",
    "    \n",
    "    4.2 Learning and predicting\n",
    "    \n",
    "    4.3 More about the feature space\n",
    "    \n",
    "    4.4 Training and testing\n",
    "    \n",
    "    4.5 Model selection (I)\n",
    "\n",
    "## PART 2: Learning concepts and theory\n",
    "\n",
    "5. What is learning?\n",
    "\n",
    "    5.1 PAC-learning\n",
    "      \n",
    "6. Inside the learning model\n",
    "\n",
    "    6.4 The human machine learning algorithm\n",
    "    \n",
    "    6.5 Model class and hypothesis space\n",
    "    \n",
    "    6.6 Objective function\n",
    "    \n",
    "    6.7 Searching/Optimization/Learning algorithm\n",
    "    \n",
    "7. Learning curves and overfitting\n",
    "\n",
    "    7.1 Learning curves\n",
    "    \n",
    "    7.2 Overfitting\n",
    "        \n",
    "8. Cures to overfitting\n",
    "\n",
    "    8.1 Model selection (II)\n",
    "    \n",
    "    8.2 Regularization\n",
    "    \n",
    "    8.3 Ensemble\n",
    "    \n",
    "9. What to do when...?\n",
    "\n",
    "## PART 3: First models\n",
    "\n",
    "10. Generative and discriminative models\n",
    "    \n",
    "    10.1\tBayesian models (Naive Bayes) and some applications.\n",
    "    \n",
    "    10.2\tSupport Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: A practical approach to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About the software\n",
    "### Scikit-Learn\n",
    "\n",
    "+ Scikit-Learn is a Machine learning library writen in Python.\n",
    "+ Simple and efficient, for both experts and non-experts.\n",
    "+ Classical, well-established machine learning algorithms.\n",
    "+ BSD 3 license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Integration in the scientific Python ecosystem\n",
    "\n",
    "The open source Python ecosystem provides a standalone, versatile and powerful scientific working environment, including:\n",
    "\n",
    "+ NumPy (for efficient manipulation of multi-dimensional arrays);\n",
    "+ SciPy (for specialized data structures (e.g., sparse matrices) and lower-level scientific algorithms),\n",
    "+ IPython (for interactive exploration),\n",
    "+ Matplotlib (for vizualization)\n",
    "+ Pandas (for data management and data analysis)\n",
    "+ (and many others...)\n",
    "\n",
    "Scikit-Learn builds upon NumPy and SciPy and complements this scientific environment with machine learning algorithms; By design, Scikit-Learn is non-intrusive, easy to use and easy to combine with other libraries. We will use Scikit-Learn as a tool for understanding machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Machine Learning?\n",
    "\n",
    "**Machine Learning** (ML) is about coding programs that automatically adjust their performance from exposure to information encoded in data. This learning is achieved via a parameterized model with tunable parameters automatically adjusted according to a performance criteria.\n",
    "\n",
    "Machine Learning can be considered a subfield of Artificial Intelligence (AI).\n",
    "\n",
    "There are three major classes of ML:\n",
    "\n",
    "   1. Supervised learning : Algorithms which learn from a training set of labeled examples (exemplars) to generalize to the set of all possible inputs. Examples of techniques in supervised learning include regression and support vector machines.\n",
    "    \n",
    "   2. Unsupervised learning : Algorithms which learn from a training set of unlableled examples, using the features of the inputs to categorize inputs together according to some statistical criteria. Examples of unsupervised learning include k-means clustering and kernel density estimation.\n",
    "    \n",
    "   3. Reinforcement learning : Algorithms that learn via reinforcement from a critic that provides information on the quality of a solution, but not on how to improve it. Improved solutions are achieved by iteratively exploring the solution space. We will not cover RL in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling the machine learning problem\n",
    "\n",
    "The first step to apply data science and machine learning is identifying an interesting question to answer. According to the type of answer we are seeking we are directly aiming for a certain set of techniques.\n",
    "\n",
    "+ If our question is answered by *YES/NO*, we are in front of a **classification** problem. Classifiers are also the techniques to use if our question admits only a discrete set of answers, i.e. we want to select among a finite number of choices.  \n",
    "\n",
    "    + Given a client profile and past activity, which are the financial products she would be most interested in?\n",
    "    \n",
    "    + Given the results of a clinical test, does this patient suffers from diabetes?\n",
    "    \n",
    "    + Given an Magnetic Resonance Image, is there a tumor in it?\n",
    "    \n",
    "    + Given the past activity associated to a credit card, is the current operation a fraud?\n",
    "    \n",
    "    + Given my skills and marks in computer science and maths, will I pass the data science course?\n",
    "\n",
    "+ If our question is a prediction of a (usually real valued) quantity, we are in front of a **regression** problem.\n",
    "\n",
    "    + Given the description of an appartment, which is the expected market value of the flat? What would the value be if the appartment has a elevator?\n",
    "    \n",
    "    + Given the past records of user activities on Apps, how long is a certain client be hocked to our App?\n",
    "    \n",
    "    + Given my skills and marks in computer science and maths, what mark will I achive?\n",
    "    \n",
    "    \n",
    "Observe that some problems can be solved using both regression and classification. As we will see later many classification algorithms are thresholded regressors. There is a certain skill in designing the correct question and this dramatically change the solution we obtain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAKE HOME PRINCIPLE:** Our first designing principle to keep in mind is that in general if a problem can be solved using a simpler question do not use a more complex one. This is an instantiation of the famous KISS principle (*Keep It Simple, Stupid!*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUIZ:** Which of the following questions correspond to a classification problem?\n",
    "\n",
    "<li> Weather forecast. \n",
    "<li> Is this behavior normal?\n",
    "<li> Where are my keys in this picture? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The supervised classification problem. A basic and guided programatic example.\n",
    "\n",
    " \n",
    "In a supervised classification problem, given a set of examples with their corresponding label, our goal is to predict the membership of a given instance to one of a predefined discrete set of classes. \n",
    "\n",
    "Formally, we can describe the problem as follows: Consider a set *training set* composed of $N$ data sample pairs $\\{(x_i,y_i)\\}, \\quad i =1,\\dots,N$ where $x_i \\in {\\bf R}^d$ is described by $d$ features, and its corresponding supervised label, e.g. in the simplest binary case $y_i = \\{-1,1\\}$. Our goal is to find a model $h:{\\bf R}^d \\rightarrow {\\bf R}$ such that given a new data sample $x$ it correctly predicts its label $y$, i.e. $h(x) = y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In machine learning we usually talk about two different steps:\n",
    "\n",
    "+ **Training**. Given a set of data instances $x$ and their corresponding label $y$ we want to learn/<span style=\"color:red\">fit</span> a model.\n",
    "\n",
    "+ **Testing or exploitation**. Given a model we want to apply it to new unseen data in order to <span style=\"color:red\">predict</span> its label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following example on how to handle **basic training** and **persistence**. We consider a visual problem in order to build up our intuition on the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The problem:** Consider the problem of handrwiten digits recognition. Given an image of a handwriten digit we want to build a classifier that recognizes the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start loading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data set.\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the data just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data format.\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "print (X.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Representing a machine learning problem in Scikit-Learn\n",
    "\n",
    "Recall the formalization of the problem where the training data set consists of $N$ data pairs $S = \\{(x_i,y_i)\\},\\; i = 1\\dots N$ where $x_iÂ \\in {\\bf R}^d$ is composed of $d$ features/descriptors and $y_i \\in \\{1,\\dots,K\\}$ is a discrete target label. In our current problem, we have $N = 1797$ data examples of handwritten numbers. Each sample is an $8\\times 8$ image. The representation of each data sample is encoded in vector. For this reason we flatten the image and reshape it to a vector with $d=64$ corresponding to the gray values/brightness of each pixel of the image. $y_i$ is the value of the target class the number belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the first digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# The original digit has been flattened, so we reshape it back to its original form\n",
    "# Check the dimensionality of the data, e.g. the first element in the data set X[0]\n",
    "print (X[0].shape)\n",
    "print (X[0])\n",
    "\n",
    "# Reshape it to 8x8 to recover the original image\n",
    "print (X[0].reshape((8,8)))\n",
    "\n",
    "\n",
    "# Show the image using scikit.image package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[6].reshape((8,8)),cmap=\"gray\",interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check some of the examples we have in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize some of the data.\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(8, 12, subplot_kw={'xticks':[], 'yticks':[]})\n",
    "for i in range(ax.size):\n",
    "    ax.flat[i].imshow(digits.data[i].reshape(8, 8),\n",
    "                      cmap=plt.cm.binary)\n",
    "fig.set_size_inches((10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem in Scikit-Learn is modeled as follows:\n",
    "\n",
    "+ Input data is structured in Numpy arrays. The size of the array is expected to be [n_samples, n_features]:\n",
    "\n",
    "    + *n_samples*: The number of samples ($N$): each sample is an item to process (e.g. classify). A sample can be a document, a picture, a sound, a video, an astronomical object, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n",
    "  \n",
    "    + *n_features*: The number of features ($d$) or distinct traits that can be used to describe each item in a quantitative manner. Features are generally real-valued, but may be boolean, discrete-valued or even cathegorical.\n",
    "\n",
    "$${\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "x_{31} & x_{32} & \\cdots & x_{3d}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{Nd}\\\\\n",
    "\\end{matrix}\n",
    "\\right]$$\n",
    "\n",
    "$${\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]$$\n",
    "    \n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional (e.g. millions of features) with most of them being zeros for a given sample. \n",
    "\n",
    "**Example** *Consider a text document representation. Given a text document we want to build a representation for it. In this case we could use as a description of the document a dictionary with all possible words in our language and create a description of the document as the number of times a certain word appears in the document. Each document is a sample and each value counting the number of times a word appear in the text is a feature. Observe that a single document will use just a handful of words. Thus there are many words not used and their representation will be zero. This is a case where scipy.sparse matrices can be useful, in that they are much more memory-efficient than numpy arrays.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set jargon\n",
    "\n",
    "Considering data arranged as in the previous section we refer to:\n",
    "\n",
    "+ the **columns** as features, attributes, dimensions, regressors, covariates, predictors, independent variables,\n",
    "+ the **rows** as instances, examples, samples.\n",
    "+ the **target** as label, outcome, response, dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Consider the following problem: *We are asked to develop a product similar to Shazzam(tm). This is, recognize the name of a song given a small sample of the music.*\n",
    "<p>\n",
    "Discuss and describe a posible feature vector for this problem with your partner.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Learning and predicting with Scikit-Learn\n",
    "\n",
    "All objects in scikit-learn share a uniform and limited API consisting of three complementary interfaces:\n",
    "\n",
    "+ an estimator interface for building and fitting models (.fit());\n",
    "+ a predictor interface for making predictions (.predict());\n",
    "+ a transformer interface for converting data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us choose a model and fit the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a classifier using .fit()\n",
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[KNN explained](https://www.youtube.com/watch?v=HVXime0nQeI&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model to disk (it can alternatively be stored in a string)\n",
    "import pickle\n",
    "ofname = open('my_classifier.pkl', 'wb')\n",
    "s = pickle.dump(knn,ofname)\n",
    "ofname.close()\n",
    "print (s)\n",
    "\n",
    "#Clear the namespace\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check we don't have the variable in the namespace. This should give a NameError\n",
    "# print(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us do some **exploitation** of the model. In this example we use the same data but in general new and unseen data is to be provided to the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "import pickle\n",
    "ofname = open('my_classifier.pkl','rb') #Open in binary format. You never know how it was saved.\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "knn = pickle.load(ofname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the prediction according to the model\n",
    "print(knn.predict(X[0,:].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the target value.\n",
    "y = digits.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicted model\n",
    "y_pred=knn.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of the classifier, prediction accuracy can used:\n",
    "$$acc  = \\frac{\\mbox{# of correct predictions}}{N}$$\n",
    "\n",
    "Each estimator has a *.score()* method that invokes the default scoring metric. In the case of k-Nearest Neighbors this is classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the performance on the training set \n",
    "# - IF YOU KNOW WHAT YOU ARE DOING YOU WILL NEVER DO THIS AGAIN!\n",
    "knn.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Put all the problem steps in one cell and check it runs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9814814814814815\n",
      "Accuracy: 0.9907407407407407\n",
      "Accuracy: 0.9907407407407407\n",
      "Accuracy: 0.9851851851851852\n",
      "Accuracy: 0.9962962962962963\n",
      "Accuracy: 0.9851851851851852\n",
      "Accuracy: 0.9814814814814815\n",
      "Accuracy: 0.9814814814814815\n",
      "Accuracy: 0.9777777777777777\n",
      "Accuracy: 0.987037037037037\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = datasets.load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Run the model 10 times\n",
    "for _ in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    accuracy = knn.score(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 More intuition about the data: The feature space\n",
    "\n",
    "When we work with data, especially in machine learning, we often start with raw values. In our case, we're dealing with images of digits, and the raw values are the gray levels of each pixel in these images. To make sense of this data and potentially improve how we distinguish between different digits, we can extract or derive new features from the raw data. These features are characteristics we compute from the data, based on our understanding of what might be important to identify the differences between classes (in this case, different digit types).\n",
    "\n",
    "#### Derived Features: Symmetry and Area\n",
    "\n",
    "For the digits dataset, we consider three specific features that might help us differentiate between digits:\n",
    "\n",
    "- **Horizontal Symmetry**: How similar is the left half of the image to the right half?\n",
    "- **Vertical Symmetry**: How similar is the top half of the image to the bottom half?\n",
    "- **Area**: The sum of all pixel values, which can be thought of as the \"weight\" of the digit.\n",
    "\n",
    "#### The Code Explained\n",
    "\n",
    "##### Visualizing Symmetry\n",
    "\n",
    "First, we're going to look at one digit and explore its symmetry:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skimage import io as io\n",
    "\n",
    "# Reshape the 8th digit in the dataset to its original 8x8 form.\n",
    "tmp = X[7].reshape((8,8))\n",
    "\n",
    "# Visualize the original digit and its horizontal mirror.\n",
    "io.imshow(tmp)\n",
    "io.show()\n",
    "io.imshow(tmp[:,::-1]) # This flips the image horizontally.\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code simply shows us the original image of a digit and its horizontally flipped version to give us a visual intuition about symmetry.\n",
    "\n",
    "**Calculating New Features:**\n",
    "\n",
    "Next, we calculate the three features (horizontal symmetry, vertical symmetry, and area) for each digit in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize an array to hold the new features for each image.\n",
    "Xnew = np.zeros((y.shape[0],3))\n",
    "\n",
    "for i in range(y.shape[0]):\n",
    "    area = sum(X[i]) # Calculate the area feature.\n",
    "    tmp = X[i].reshape((8,8))\n",
    "    symH = tmp * tmp[:,::-1] # Calculate horizontal symmetry.\n",
    "    symV = tmp * tmp[::-1,:] # Calculate vertical symmetry.\n",
    "    \n",
    "    # Store the calculated features.\n",
    "    Xnew[i,:] = [sum(symH.flatten()), area, sum(symV.flatten())]\n",
    "\n",
    "print(Xnew)\n",
    "print(Xnew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this dataset for later use\n",
    "import pickle\n",
    "ofname = open('my_digits_data.pkl', 'wb')\n",
    "s = pickle.dump([Xnew,y],ofname)\n",
    "ofname.close()\n",
    "print ('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script transforms each image to compute and store our three chosen features: the sums of the products of the image and its horizontal and vertical mirrors (for symmetry) and the sum of pixel values (for area).\n",
    "\n",
    "Visualizing the Feature Space\n",
    "Finally, we visualize how two digits, 0 and 6, differ in this new feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find indices of digits 0 and 6.\n",
    "idxA = y==0\n",
    "idxB = y==6\n",
    "\n",
    "# Choose which features to plot.\n",
    "feature1 = 1\n",
    "feature2 = 2\n",
    "\n",
    "# Plot the features for digits 0 and 6.\n",
    "plt.figure()\n",
    "plt.scatter(Xnew[idxA, feature1], Xnew[idxA, feature2], c='blue', alpha=0.2)\n",
    "plt.scatter(Xnew[idxB, feature1], Xnew[idxB, feature2], c='red', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates a scatter plot to visually compare the selected features for digits 0 and 6. The plot helps us see if and how these digits can be distinguished based on the new features we've created.\n",
    "\n",
    "Conclusion\n",
    "By deriving new features from the raw data, we can often uncover patterns that help us differentiate between classes more effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Change feature1 and feature2 axis $\\in \\{0,1,2\\}$ and select the most suitable view for classification purposes. Why did you select that view?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of using knowledge domain information in order to create discriminant features is called <span style=\"color:red\">feature extraction</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data vs. Feature Extraction\n",
    "\n",
    "When working with machine learning models, we often encounter two primary approaches to preparing our data: using raw data directly or extracting features from this data. Both approaches have their advantages and drawbacks, which are crucial to understand for effective model development.\n",
    "\n",
    "#### Raw Data\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- **Accessibility:** Raw data can be used directly without the need for additional processing or transformation. This approach does not require domain-specific knowledge, making it straightforward and accessible to practitioners at all levels.\n",
    "\n",
    "**Drawbacks:**\n",
    "\n",
    "- **Redundancy and Dimensionality:** Raw data is often highly redundant, containing information that may not be necessary for making accurate predictions or classifications. This redundancy typically results in very large dimensional spaces, which can complicate model training and lead to longer computation times.\n",
    "- **Unknown Discriminability:** With raw data, it's not always clear which features are important for distinguishing between classes. This lack of clarity can hinder a model's ability to learn effectively, as it may focus on irrelevant features.\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- **Discriminant Information:** Feature extraction aims to identify and capture the most relevant information in the data for the task at hand. By focusing on discriminant information, models can often achieve better performance with less data.\n",
    "- **Reduced Dimensionality and Complexity:** Through feature extraction, data is transformed into a lower-dimensional space that reflects the most important aspects of the original data. This reduction in dimensionality and complexity can lead to faster training times and more efficient models.\n",
    "\n",
    "**Drawbacks:**\n",
    "\n",
    "- **Domain Knowledge Required:** Unlike using raw data, feature extraction often requires domain-specific knowledge to identify which features are likely to be informative. This requirement can make feature extraction less accessible to those without expertise in the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Train a new classifier on the new training set and check its training performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXERCISE#TODO NOW!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring performance\n",
    "\n",
    "There are different criteria for measuring performance of a classifier and the most adequate metric is usually problem dependent. When no prior information on the problem in given, we usually use classification accuracy. When we are in front of a multi-class problem (there are many classes to choose from) we may use the confusion matrix. The elements of the confusion matrix $M$ are defined as follows,\n",
    "\n",
    "$$M(i,j) = \\mbox{# of samples from class j predicted as class i}$$\n",
    "\n",
    "[confusion matrix](https://www.youtube.com/watch?v=Kdsp6soqA7o&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "Let us check these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(y, y_pred):\n",
    "    # Generate the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Adding tick marks and labels for clarity\n",
    "    classes = np.unique(y)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Adding text annotations to each cell in the confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Example usage\n",
    "# Assume y and y_pred are defined elsewhere in your code\n",
    "print(\"Classification accuracy:\", metrics.accuracy_score(y, y_pred))\n",
    "plot_confusion_matrix(y, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Which are the classes with more confusion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training and testing. \n",
    "#### More intuition behind the learning process\n",
    "\n",
    "Understanding the learning process in machine learning is crucial for effectively applying models to solve problems. The code snippet below demonstrates how to train a K Nearest Neighbors (KNN) classifier using a featurized dataset, make predictions, and evaluate the model's performance. This process is fundamental to machine learning and provides insights into how models learn from data to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# Create an instance of the KNeighborsClassifier with 1 neighbor.\n",
    "# This means the algorithm will consider the closest neighbor to make a prediction.\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Train the classifier on the dataset.\n",
    "# Here, Xnew represents the featurized version of the original data, and y represents the labels.\n",
    "# The .fit() method is used to train the model using the provided features and labels.\n",
    "knn.fit(Xnew, y)\n",
    "\n",
    "# Make predictions on the dataset using the trained model.\n",
    "# The .predict() method is applied to the featurized data Xnew, generating predictions yhat.\n",
    "yhat = knn.predict(Xnew)\n",
    "\n",
    "# Print the classification accuracy of the model.\n",
    "# The accuracy_score function compares the predicted labels (yhat) with the true labels (y) to calculate the accuracy.\n",
    "print(\"Classification accuracy:\", metrics.accuracy_score(yhat, y))\n",
    "\n",
    "# Plot the confusion matrix for the true labels and predicted labels.\n",
    "# This function visually represents the performance of the classification model,\n",
    "# showing how many predictions are correct (diagonal) versus incorrect (off-diagonal).\n",
    "plot_confusion_matrix(y, yhat)\n",
    "\n",
    "# Note: The featurized model (Xnew) is used here instead of the original dataset,\n",
    "# indicating that preprocessing steps or feature extraction techniques have been applied to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION :** \n",
    "\n",
    "<li> Which is the accuracy of this classifier on the training set?\n",
    "<li> Do we expect to work better than the former one on new data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Classifier Performance with Train/Test Splits\n",
    "\n",
    "Until now, we've evaluated our classifier's performance using the same dataset on which it was trained. While this can provide some insights into how well the model has learned the training data, it doesn't accurately represent how the model will perform on new, unseen data. In real-world applications, we expect our model to make predictions on data it has never encountered during the training phase.\n",
    "\n",
    "#### The Importance of Train/Test Splits\n",
    "\n",
    "To simulate a more realistic scenario and better understand the model's generalization ability, we split our dataset into two separate sets:\n",
    "\n",
    "- **Training set:** This portion of the data is used to train the model. It learns from this data, adjusting its parameters to fit the given input features to the corresponding target values.\n",
    "- **Testing set:** This set is kept separate from the training process. After the model has been trained, we use this data to evaluate its performance. The testing set acts as a proxy for new, unseen data.\n",
    "\n",
    "#### Why Split?\n",
    "\n",
    "Splitting the data helps us to:\n",
    "\n",
    "1. **Assess generalization:** By evaluating the model on data it hasn't seen before, we can measure how well it generalizes to new examples. This is a better indicator of its real-world performance.\n",
    "2. **Detect overfitting:** If a model performs exceptionally well on the training data but poorly on the testing data, it's likely overfitting. Overfitting happens when a model learns the noise in the training data instead of the underlying pattern, making it perform poorly on any data outside the training set.\n",
    "3. **Tune parameters:** The train/test split allows us to adjust the model's parameters and select the best model configuration. By comparing the model's performance on the training and testing sets, we can make informed decisions about parameter settings and model choices.\n",
    "\n",
    "#### Implementing Train/Test Splits\n",
    "\n",
    "In practice, the dataset is randomly split into training and testing sets, often with ratios like 70%/30%, 80%/20%, or similar, depending on the dataset size and the specific problem. Tools like `train_test_split` from `sklearn.model_selection` can automate this process, ensuring that the data is randomly and appropriately divided.\n",
    "\n",
    "By adopting this approach, we ensure that our model evaluation is more robust and indicative of how the model will perform in practical applications, thereby increasing our confidence in its predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the workspace to ensure a clean environment\n",
    "%reset -f\n",
    "\n",
    "# Load the digits dataset from a pickle file\n",
    "import pickle\n",
    "\n",
    "# Open the file containing the dataset\n",
    "with open('my_digits_data.pkl', 'rb') as ofname:\n",
    "    # Load the dataset from the file\n",
    "    data = pickle.load(ofname)\n",
    "    # Assign features to X and target labels to y\n",
    "    X, y = data[0], data[1]\n",
    "\n",
    "# Prepare the dataset for a realistic simulation: randomize and split it into training and testing subsets\n",
    "import numpy as np\n",
    "\n",
    "# Randomly permute a sequence of indices based on the size of y\n",
    "perm = np.random.permutation(y.size)\n",
    "\n",
    "# Define the proportion of the dataset to allocate for training\n",
    "PRC = 0.7\n",
    "# Calculate the split point for dividing the dataset\n",
    "split_point = int(np.ceil(y.shape[0] * PRC))\n",
    "\n",
    "# Split the dataset into training and testing sets based on the calculated split point\n",
    "X_train = X[perm[:split_point]]\n",
    "y_train = y[perm[:split_point]]\n",
    "\n",
    "X_test = X[perm[split_point:]]\n",
    "y_test = y[perm[split_point:]]\n",
    "\n",
    "# Output the shapes of the training and testing datasets for verification\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Train a K-Nearest Neighbors classifier on the training data\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Initialize the classifier with 1 neighbor for simplicity\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set and evaluate performance\n",
    "yhat = knn.predict(X_train)\n",
    "\n",
    "# Import necessary libraries for performance evaluation\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print training statistics\n",
    "print(\"\\nTRAINING STATS:\")\n",
    "print(\"Classification accuracy:\", metrics.accuracy_score(yhat, y_train))\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(np.unique(y_train)))\n",
    "    plt.xticks(tick_marks, np.unique(y_train), rotation=45)\n",
    "    plt.yticks(tick_marks, np.unique(y_train))\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Generate the confusion matrix from the training data predictions\n",
    "cm = metrics.confusion_matrix(y_train, yhat)\n",
    "# Call the function to plot the improved confusion matrix\n",
    "plot_confusion_matrix(cm, \"Training Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test set to evaluate performance\n",
    "yhat = knn.predict(X_test)\n",
    "\n",
    "# Print testing statistics\n",
    "print(\"TESTING STATS:\")\n",
    "print(\"Classification accuracy:\", metrics.accuracy_score(yhat, y_test))\n",
    "\n",
    "# Function to plot confusion matrix with annotations for clarity\n",
    "def plot_confusion_matrix_with_numbers(cm, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix with the actual counts displayed on the matrix for better clarity.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, np.unique(y_test), rotation=45)\n",
    "    plt.yticks(tick_marks, np.unique(y_test))\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'  # Format as decimal integer\n",
    "    thresh = cm.max() / 2.  # Threshold for text color based on background\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Generate the confusion matrix from the test data predictions\n",
    "cm_test = metrics.confusion_matrix(y_test, yhat)\n",
    "\n",
    "# Plot the improved confusion matrix with numbers for the test data\n",
    "plot_confusion_matrix_with_numbers(cm_test, \"Testing Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Variability in Model Performance\n",
    "\n",
    "When evaluating a model's performance, particularly in machine learning, it's important to recognize that the results can vary each time the process is executed. This variation is due to several factors, including the randomness in the splitting of the dataset into training and testing sets, and the inherent stochastic nature of many learning algorithms.\n",
    "\n",
    "#### Why Does Performance Vary?\n",
    "\n",
    "- **Data Splitting:** Each time we randomly split the dataset into training and testing sets, the model is exposed to slightly different data during training. This can lead to variations in how well the model learns and generalizes.\n",
    "- **Model Initial Conditions:** For algorithms that involve random initialization (e.g., starting weights in neural networks), different initial conditions can lead to different learning paths and outcomes.\n",
    "- **Sampling Variance:** The subset of data chosen for training and testing might not fully represent the overall distribution of the dataset, leading to variations in performance metrics.\n",
    "\n",
    "#### Simulating Real-World Conditions\n",
    "\n",
    "To more accurately approximate the test error and account for these variations, we can employ a technique known as *cross-validation*. More simply, however, we can repeat the process of splitting, training, and testing the model multiple times, each time with a different random split. By averaging the performance metrics across these iterations, we can obtain a more stable and reliable estimate of the model's performance.\n",
    "\n",
    "#### Implementing Multiple Iterations\n",
    "\n",
    "Let's enhance our simulation by executing the train-test split, training, and evaluation process multiple times. After each iteration, we'll record the model's performance metrics. Once all iterations are complete, we'll calculate the average performance. This approach gives us a more nuanced understanding of how our model is likely to perform on unseen data, making our evaluation more robust and realistic.\n",
    "\n",
    "This process not only helps in mitigating the effects of randomness and variance but also provides insights into the consistency and reliability of the model under different conditions. By understanding and applying these concepts, we can make more informed decisions about model selection, tuning, and deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The splitting can be done using the tools provided by sklearn:\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "PRC = 0.3\n",
    "acc=np.zeros((10,))\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=PRC)\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train,y_train)\n",
    "    yhat=knn.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat, y_test)\n",
    "acc.shape=(1,10)\n",
    "print (\"Mean expected error: \"+str(np.mean(acc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarifying Error Metrics in Machine Learning\n",
    "\n",
    "In the realm of machine learning and statistical learning theory, it's essential to quantify the performance of our models. To do this, we introduce specific nomenclature for the error metrics we compute during model training and evaluation.\n",
    "\n",
    "#### In-Sample Error ($E_{\\text{in}}$)\n",
    "\n",
    "- **Definition:** The in-sample error, also known as the training error, measures the error over all observed data samples within the training set. It reflects how well the model fits the data it was trained on.\n",
    "- **Formula:** The mathematical representation is given by:\n",
    "\n",
    "  $$E_{\\text{in}} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} e(x_i,y_i)$$\n",
    "\n",
    "  where $N$ is the number of samples in the training set, and $e(x_i,y_i)$ denotes the error of the prediction on the $i$-th sample.\n",
    "\n",
    "#### Out-of-Sample Error ($E_{\\text{out}}$)\n",
    "\n",
    "- **Definition:** The out-of-sample error, or generalization error, measures the expected error on unseen data. This metric is crucial as it indicates the model's ability to generalize beyond the training data.\n",
    "- **Approximation:** We approximate $E_{\\text{out}}$ by holding out a portion of the training data for testing purposes, thus not exposing the model to this data during training.\n",
    "\n",
    "  $$E_{\\text{out}}=\\mathbb{E}_{x,y}(e(x,y))$$\n",
    "\n",
    "  Here, the expectation $\\mathbb{E}_{x,y}$ reflects the average error over all possible unseen samples.\n",
    "\n",
    "#### Instantaneous Error ($e(x_i,y_i)$)\n",
    "\n",
    "- **Definition:** To measure the error for individual predictions, we define the instantaneous error. This metric assesses the error on a single data point.\n",
    "- **Example:** In the context of classification, we might use the indicator function to evaluate whether a sample is correctly classified:\n",
    "\n",
    "  $$e(x_i,y_i) = I[h(x_i)=y_i] =\\left\\{\n",
    "  \\begin{align}\n",
    "  1 & \\text{ if } h(x_i)=y_i\\\\\n",
    "  0 & \\text{ otherwise } \\\\\n",
    "  \\end{align}\n",
    "  \\right.$$\n",
    "\n",
    "  where $h(x_i)$ is the predicted label for sample $x_i$, and $y_i$ is the true label.\n",
    "\n",
    "#### Understanding the Relationship Between $E_{\\text{in}}$ and $E_{\\text{out}}$\n",
    "\n",
    "It's a fundamental principle that the out-of-sample error is expected to be greater than or equal to the in-sample error:\n",
    "\n",
    "$$E_{\\text{out}} \\geq E_{\\text{in}}$$\n",
    "\n",
    "This inequality underscores the challenge of overfitting, where a model might perform exceptionally well on the training data ($E_{\\text{in}}$ is low) but poorly on new, unseen data ($E_{\\text{out}}$ is high). The primary goal in model training is to minimize $E_{\\text{out}}$, ensuring that our model generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Selection (I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of machine learning, **model selection** is a critical process where we determine the most suitable classifier or model for a specific application. This decision is based on the model's performance, typically evaluated using the expected error on a test set.\n",
    "\n",
    "#### Why Model Selection Matters\n",
    "\n",
    "The primary goal of model selection is to identify the model that best generalizes to unseen data. Since different models have varying strengths and weaknesses depending on the nature of the data and the task at hand, selecting the \"best\" model is crucial for achieving high performance.\n",
    "\n",
    "#### Simplistic Model Selection Scenario\n",
    "\n",
    "Consider a scenario where we have a collection of different classifiers at our disposal. The objective is straightforward: to select the classifier that performs the best according to a predetermined metric, usually the one with the lowest error rate on the test set.\n",
    "\n",
    "#### Steps for Model Selection\n",
    "\n",
    "1. **Training Multiple Classifiers:** Begin by training each candidate model on the same training dataset. This ensures that each model learns from the same information.\n",
    "2. **Evaluating on the Test Set:** Next, evaluate each model's performance on a separate test set. This test set should not have been seen by the models during training, ensuring that our evaluation reflects each model's ability to generalize.\n",
    "3. **Comparing Error Rates:** Calculate the error rate (or any other relevant performance metric) for each model on the test set. The error rate gives us a quantitative measure of how often the model makes incorrect predictions.\n",
    "4. **Selecting the Best Model:** Finally, select the model with the lowest error rate on the test set. This model is considered the \"best\" among the candidates for our specific application.\n",
    "\n",
    "#### Considerations\n",
    "\n",
    "- **Error Rate as a Metric:** While using the error rate is common, other metrics such as precision, recall, F1 score, or AUC might be more appropriate depending on the application's specific requirements.\n",
    "- **Validation Sets:** In addition to a test set, a validation set can also be used during model selection. This allows for the adjustment of model hyperparameters without using the test set, which should be reserved for the final evaluation.\n",
    "- **Complexity and Performance:** It's essential to balance model complexity with performance. A more complex model might yield a slightly lower error rate but at the cost of interpretability, computational efficiency, or the risk of overfitting.\n",
    "\n",
    "By carefully following the model selection process, we can confidently choose a classifier that offers the best trade-off between accuracy, complexity, and generalizability for our application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from sklearn for model selection, classifiers, and metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Ensure numpy is imported for array operations\n",
    "\n",
    "# Set the proportion of the dataset to be used for testing\n",
    "PRC = 0.1\n",
    "\n",
    "# Initialize an array to store the accuracy results for each classifier across iterations\n",
    "acc_r = np.zeros((10,4))\n",
    "\n",
    "# Repeat the experiment 10 times to get a distribution of performance metrics\n",
    "for i in range(10):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=PRC)\n",
    "    \n",
    "    # Initialize classifiers with specific configurations\n",
    "    nn1 = neighbors.KNeighborsClassifier(n_neighbors=1)  # 1-Nearest Neighbor classifier\n",
    "    nn3 = neighbors.KNeighborsClassifier(n_neighbors=3)  # 3-Nearest Neighbors classifier\n",
    "    svc = svm.SVC()  # Support Vector Machine classifier\n",
    "    dt = tree.DecisionTreeClassifier()  # Decision Tree classifier\n",
    "    \n",
    "    # Train each classifier on the training set\n",
    "    nn1.fit(X_train, y_train)\n",
    "    nn3.fit(X_train, y_train)\n",
    "    svc.fit(X_train, y_train)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels for the test set using each trained classifier\n",
    "    yhat_nn1 = nn1.predict(X_test)\n",
    "    yhat_nn3 = nn3.predict(X_test)\n",
    "    yhat_svc = svc.predict(X_test)\n",
    "    yhat_dt = dt.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the accuracy for each classifier\n",
    "    acc_r[i][0] = metrics.accuracy_score(yhat_nn1, y_test)\n",
    "    acc_r[i][1] = metrics.accuracy_score(yhat_nn3, y_test)\n",
    "    acc_r[i][2] = metrics.accuracy_score(yhat_svc, y_test)\n",
    "    acc_r[i][3] = metrics.accuracy_score(yhat_dt, y_test)\n",
    "\n",
    "# Visualize the accuracy results for each classifier using a boxplot\n",
    "plt.boxplot(acc_r);\n",
    "\n",
    "# Overlay individual accuracy results as red dots for visual comparison\n",
    "for i in range(4):\n",
    "    xderiv = (i+1)*np.ones(acc_r[:,i].shape) + (np.random.rand(10,)-0.5) * 0.1\n",
    "    plt.plot(xderiv, acc_r[:,i], 'ro', alpha=0.3)\n",
    "    \n",
    "# Customize the plot with appropriate labels\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['1-NN', '3-NN', 'SVM', 'Decision Tree'])\n",
    "\n",
    "plt.xlabel('Classifier Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classifier Performance Comparison')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Cross-Validation Techniques\n",
    "\n",
    "Cross-validation is a cornerstone method in machine learning for assessing how the results of a statistical analysis will generalize to an independent dataset. It's especially useful in scenarios where the goal is to predict, and one wants to estimate how accurately a predictive model will perform in practice. The process we've discussed is a form of cross-validation, which encompasses several different methods, including **leave-one-out** and **K-fold cross-validation**.\n",
    "\n",
    "#### Leave-One-Out Cross-Validation\n",
    "\n",
    "- **How it Works:** In leave-one-out cross-validation (LOOCV), for a dataset with $N$ samples, the model is trained using $N-1$ samples and tested on the remaining single sample. This process is repeated $N$ times, with each of the $N$ samples used exactly once as the test set.\n",
    "- **Advantages:** LOOCV uses nearly all the data for training, making it an excellent option for small datasets.\n",
    "- **Drawbacks:** It can be computationally expensive for larger datasets since the model needs to be retrained from scratch $N$ times.\n",
    "\n",
    "#### K-Fold Cross-Validation\n",
    "\n",
    "- **How it Works:** In K-fold cross-validation, the training set is randomly partitioned into $K$ equal-sized subsamples. Of the $K$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining $K-1$ subsamples are used as training data. The cross-validation process is then repeated $K$ times (the folds), with each of the $K$ subsamples used exactly once as the validation data.\n",
    "- **Advantages:** This method is less computationally expensive than LOOCV, especially for large datasets. It also allows for a more thorough mixing of the data, since each fold is used for both training and validation.\n",
    "- **Estimating Confidence:** K-fold cross-validation provides not only an estimation of the model's performance but also allows for the computation of a confidence interval around the estimated performance using the variation in performance across the folds.\n",
    "\n",
    "#### Choosing Between Cross-Validation Methods\n",
    "\n",
    "- **Dataset Size:** For smaller datasets, LOOCV might be preferable due to its intensive use of data for training. For larger datasets, K-fold cross-validation is often more practical due to its reduced computational load.\n",
    "- **Variance and Bias:** K-fold cross-validation tends to have lower variance as an estimator of the test error since multiple testing sets are used and averaged over. LOOCV, by testing on only one example at a time, can have higher variance.\n",
    "- **Confidence Intervals:** The ability to estimate confidence intervals in K-fold cross-validation helps in understanding the reliability of the model evaluation process.\n",
    "\n",
    "In summary, the choice of cross-validation technique can significantly impact the efficiency and effectiveness of model selection and evaluation processes. The key is to balance the computational cost with the benefits of each method's approach to handling variance and leveraging available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, tree, svm, metrics\n",
    "\n",
    "# Initialize an array to hold the accuracy scores for each fold and model\n",
    "acc = np.zeros((10,4))\n",
    "\n",
    "# Create a KFold object for 10-fold cross-validation\n",
    "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# Get the number of splits (this line can be omitted as we don't use its result directly)\n",
    "kf.get_n_splits()\n",
    "\n",
    "# Loop counter\n",
    "i = 0\n",
    "# Iterate over each fold defined by KFold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and testing sets based on the current fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialize classifiers\n",
    "    nn1 = neighbors.KNeighborsClassifier(n_neighbors=1)  # 1-nearest neighbor\n",
    "    nn3 = neighbors.KNeighborsClassifier(n_neighbors=3)  # 3-nearest neighbors\n",
    "    svc = svm.SVC()  # Support vector machine\n",
    "    dt = tree.DecisionTreeClassifier()  # Decision tree\n",
    "    \n",
    "    # Train each classifier on the training set\n",
    "    nn1.fit(X_train, y_train)\n",
    "    nn3.fit(X_train, y_train)\n",
    "    svc.fit(X_train, y_train)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing set\n",
    "    yhat_nn1 = nn1.predict(X_test)\n",
    "    yhat_nn3 = nn3.predict(X_test)\n",
    "    yhat_svc = svc.predict(X_test)\n",
    "    yhat_dt = dt.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the accuracy for each classifier\n",
    "    acc[i][0] = metrics.accuracy_score(yhat_nn1, y_test)\n",
    "    acc[i][1] = metrics.accuracy_score(yhat_nn3, y_test)\n",
    "    acc[i][2] = metrics.accuracy_score(yhat_svc, y_test)\n",
    "    acc[i][3] = metrics.accuracy_score(yhat_dt, y_test)\n",
    "    # Increment the loop counter\n",
    "    i += 1\n",
    "\n",
    "# Plotting the accuracy scores as a boxplot for each classifier\n",
    "plt.boxplot(acc);\n",
    "# Overlay individual accuracy scores as red dots for better visualization\n",
    "for i in range(4):\n",
    "    xderiv = (i+1)*np.ones(acc[:,i].shape) + (np.random.rand(10,) - 0.5) * 0.1\n",
    "    plt.plot(xderiv, acc[:,i], 'ro', alpha=0.3)\n",
    "\n",
    "# Setting the labels for each classifier on the x-axis\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['1-NN', '3-NN', 'SVM', 'Decision Tree'])\n",
    "\n",
    "plt.xlabel('Classifier Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('10-Fold Cross-Validation Results')\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for fun let us put both plots together\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "for i in range(4):\n",
    "    plt.boxplot([acc[:,i], acc_r[:,i]], positions = [2*i+1,2*i+2], widths = 0.6)\n",
    "    xderiv = (2*i+1)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc[:,i],'ro',alpha=0.3)\n",
    "    xderiv = (2*i+2)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc_r[:,i],'bo',alpha=0.3)\n",
    "# set axes limits and labels\n",
    "plt.xlim(0,9)\n",
    "plt.ylim(0,0.4)\n",
    "ax.set_xticklabels(['1-NN','1-NN','3-NN','3-NN','SVM','SVM','Decission Tree','Decission Tree'],rotation = 45, ha=\"right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Scikit-learn's estimator interface\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all methods,\n",
    "and we'll see examples of these below. Given a scikit-learn *estimator*\n",
    "object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all Estimators**\n",
    "  + `model.fit()` : fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "    For unsupervised learning applications, this accepts only a single argument,\n",
    "    the data `X` (e.g. `model.fit(X)`).\n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "- Available in **unsupervised estimators**\n",
    "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
    "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
    "    on the unsupervised model.\n",
    "  + `model.fit_transform()` : some estimators implement this method,\n",
    "    which more efficiently performs a fit and a transform on the same input data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
